vscode-remote://vscode-01jncdgx9p5y0tyjm2bbeyx6mz.studio.lightning.ai/teamspace/studios/this_studio/Classification-d-image-avec-plusieurs-modele/HW1_2025/assignment1_release/main.ipynb {"mtime":1741324005893,"ctime":1741281418602,"size":75016,"etag":"3drddcffi2g1r","orphaned":false,"typeId":"notebook/jupyter-notebook/jupyter-notebook"}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTv0D26B9W2h"
   },
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qFHMMDtSwuW4"
   },
   "outputs": [],
   "source": [
    "#@title Mount your Google Drive\n",
    "# If you run this notebook locally or on a cluster (i.e. not on Google Colab)\n",
    "# you can delete this cell which is specific to Google Colab. You may also\n",
    "# change the paths for data/logs in Arguments below.\n",
    "# %matplotlib inline\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "oODLwt1QzgGa"
   },
   "outputs": [],
   "source": [
    "#@title Link your assignment folder & install requirements\n",
    "#@markdown Enter the path to the assignment folder in your Google Drive\n",
    "# If you run this notebook locally or on a cluster (i.e. not on Google Colab)\n",
    "# you can delete this cell which is specific to Google Colab. You may also\n",
    "# change the paths for data/logs in Arguments below.\n",
    "# import sys\n",
    "# import os\n",
    "# import shutil\n",
    "# import warnings\n",
    "\n",
    "# folder = \"Your path to assignment folder\" #@param {type:\"string\"}\n",
    "# !ln -Ts \"$folder\" /content/assignment 2> /dev/null\n",
    "\n",
    "# # Add the assignment folder to Python path\n",
    "# if '/content/assignment' not in sys.path:\n",
    "#   sys.path.insert(0, '/content/assignment')\n",
    "\n",
    "# # Check if CUDA is available\n",
    "# import torch\n",
    "# if not torch.cuda.is_available():\n",
    "#   warnings.warn('CUDA is not available.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "dt3NTvpsy4Oc"
   },
   "source": [
    "### Running on GPU\n",
    "For this assignment, it will be necessary to run your experiments on GPU. To make sure the notebook is running on GPU, you can change the notebook settings with\n",
    "* (EN) `Edit > Notebook Settings`\n",
    "* (FR) `Modifier > Param√®tres du notebook`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "RLVSmv9HoMH5"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "from torch import optim\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "from utils import seed_experiment, to_device, cross_entropy_loss, compute_accuracy\n",
    "from config import get_config_parser\n",
    "import json\n",
    "from mlp import MLP\n",
    "from resnet18 import ResNet18\n",
    "from mlpmixer import MLPMixer\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import os\n",
    "from utils import generate_plots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gZy1J-0OroLg"
   },
   "source": [
    "# Local Test\n",
    "Before run the experiment, here are some local test cases you can run for sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "wLEVxwLlroLh"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_linear_attributes (test.TestLinear) ... ok\n",
      "test_linear_forward (test.TestLinear) ... ok\n",
      "test_activation (test.TestMLP) ... ok\n",
      "test_forward (test.TestMLP) ... ok\n",
      "test_mlp (test.TestMLP) ... ok\n",
      "test_mixer_block (test.TestMLPMixer) ... ok\n",
      "test_mlpmixer (test.TestMLPMixer) ... ok\n",
      "test_patch_emb (test.TestMLPMixer) ... ok\n",
      "test_basic_block (test.TestResNet) ... ok\n",
      "test_basic_block2 (test.TestResNet) ... ok\n",
      "test_resnet (test.TestResNet) ... ok\n",
      "test_ce_loss (test.TestUtils) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 12 tests in 0.596s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=12 errors=0 failures=0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "import test\n",
    "suite = unittest.TestLoader().loadTestsFromModule(test)\n",
    "unittest.TextTestRunner(verbosity=2).run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-PtvL_yKp3PW"
   },
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWiJme7XaLiR"
   },
   "source": [
    "Below we define a few default arguments to get you started with your experiments. You are encouraged to modify the function `main_entry()`, as well as these arguments, to fit your needs (e.g. changing hyperparameters, the optimizer, adding regularizations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "YUrqebfCobD1"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Arguments:\n",
    "  # Data\n",
    "  batch_size: int = 128\n",
    "  # Model\n",
    "  model: str = 'mlp'  # [mlp, resnet18, mlpmixer]\n",
    "  model_config: str = \"./model_configs/mlp.json\" # path to model config json file\n",
    "\n",
    "  # Optimization\n",
    "  optimizer: str = 'adamw'  # [sgd, momentum, adam, adamw]\n",
    "  epochs: int = 15\n",
    "  lr: float = 1e-3\n",
    "  momentum: float = 0.9\n",
    "  weight_decay: float = 5e-4\n",
    "\n",
    "  # Experiment\n",
    "  logdir: str = '/content/assignment/logs'\n",
    "  seed: int = 42\n",
    "\n",
    "  # Miscellaneous\n",
    "  device: str = 'cuda'\n",
    "  visualize : bool = False\n",
    "  print_every: int = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "g2rjoY-5phTY"
   },
   "outputs": [],
   "source": [
    "# Main code entry. Train the model and save the logs\n",
    "from main import train, evaluate\n",
    "def main_entry(args):\n",
    "    # Check for the device\n",
    "    if (args.device == \"cuda\") and not torch.cuda.is_available():\n",
    "        warnings.warn(\n",
    "            \"CUDA is not available, make that your environment is \"\n",
    "            \"running on GPU (e.g. in the Notebook Settings in Google Colab). \"\n",
    "            'Forcing device=\"cpu\".'\n",
    "        )\n",
    "        args.device = \"cpu\"\n",
    "\n",
    "    if args.device == \"cpu\":\n",
    "        warnings.warn(\n",
    "            \"You are about to run on CPU, and might run out of memory \"\n",
    "            \"shortly. You can try setting batch_size=1 to reduce memory usage.\"\n",
    "        )\n",
    "\n",
    "    # Seed the experiment, for repeatability\n",
    "    seed_experiment(args.seed)\n",
    "\n",
    "    test_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                     transforms.Normalize([0.49139968, 0.48215841, 0.44653091], [0.24703223, 0.24348513, 0.26158784])\n",
    "                                     ])\n",
    "    # For training, we add some augmentation. Networks are too powerful and would overfit.\n",
    "    train_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                          transforms.RandomResizedCrop((32,32),scale=(0.8,1.0),ratio=(0.9,1.1)),\n",
    "                                          transforms.ToTensor(),\n",
    "                                          transforms.Normalize([0.49139968, 0.48215841, 0.44653091], [0.24703223, 0.24348513, 0.26158784])\n",
    "                                        ])\n",
    "    # Loading the training dataset. We need to split it into a training and validation part\n",
    "    # We need to do a little trick because the validation set should not use the augmentation.\n",
    "    train_dataset = CIFAR10(root='./data', train=True, transform=train_transform, download=True)\n",
    "    val_dataset = CIFAR10(root='./data', train=True, transform=test_transform, download=True)\n",
    "    train_set, _ = torch.utils.data.random_split(train_dataset, [45000, 5000])\n",
    "    _, val_set = torch.utils.data.random_split(val_dataset, [45000, 5000])\n",
    "\n",
    "    # Loading the test set\n",
    "    test_set = CIFAR10(root='./data', train=False, transform=test_transform, download=True)\n",
    "    \n",
    "    # Load model\n",
    "    print(f'Build model {args.model.upper()}...')\n",
    "    if args.model_config is not None:\n",
    "        print(f'Loading model config from {args.model_config}')\n",
    "        with open(args.model_config) as f:\n",
    "            model_config = json.load(f)\n",
    "    else:\n",
    "        raise ValueError('Please provide a model config json')\n",
    "    print(f'########## {args.model.upper()} CONFIG ################')\n",
    "    for key, val in model_config.items():\n",
    "        print(f'{key}:\\t{val}')\n",
    "    print('############################################')\n",
    "    model_cls = {'mlp': MLP, 'resnet18': ResNet18, 'mlpmixer': MLPMixer}[args.model]\n",
    "    model = model_cls(**model_config)\n",
    "    model.to(args.device)\n",
    "    \n",
    "    # Optimizer\n",
    "    if args.optimizer == \"adamw\":\n",
    "        optimizer = optim.AdamW(\n",
    "            model.parameters(), lr=args.lr, weight_decay=args.weight_decay\n",
    "        )\n",
    "    elif args.optimizer == \"adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    elif args.optimizer == \"sgd\":\n",
    "        optimizer = optim.SGD(\n",
    "            model.parameters(), lr=args.lr, weight_decay=args.weight_decay\n",
    "        )\n",
    "    elif args.optimizer == \"momentum\":\n",
    "        optimizer = optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=args.lr,\n",
    "            momentum=args.momentum,\n",
    "            weight_decay=args.weight_decay,\n",
    "        )\n",
    "    \n",
    "    print(\n",
    "        f\"Initialized {args.model.upper()} model with {sum(p.numel() for p in model.parameters())} \"\n",
    "        f\"total parameters, of which {sum(p.numel() for p in model.parameters() if p.requires_grad)} are learnable.\"\n",
    "    )\n",
    "\n",
    "    train_losses, valid_losses = [], []\n",
    "    train_accs, valid_accs = [], []\n",
    "    train_times, valid_times = [], []\n",
    "    \n",
    "    # We define a set of data loaders that we can use for various purposes later.\n",
    "    train_dataloader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True, drop_last=True, pin_memory=True, num_workers=4)\n",
    "    valid_dataloader = DataLoader(val_set, batch_size=args.batch_size, shuffle=False, drop_last=False, num_workers=4)\n",
    "    test_dataloader = DataLoader(test_set, batch_size=args.batch_size, shuffle=False, drop_last=False, num_workers=4)\n",
    "    for epoch in range(args.epochs):\n",
    "        tqdm.write(f\"====== Epoch {epoch} ======>\")\n",
    "        loss, acc, wall_time = train(epoch, model, train_dataloader, optimizer,args)\n",
    "        train_losses.append(loss)\n",
    "        train_accs.append(acc)\n",
    "        train_times.append(wall_time)\n",
    "\n",
    "        loss, acc, wall_time = evaluate(epoch, model, valid_dataloader,args)\n",
    "        valid_losses.append(loss)\n",
    "        valid_accs.append(acc)\n",
    "        valid_times.append(wall_time)\n",
    "\n",
    "    test_loss, test_acc, test_time = evaluate(\n",
    "        epoch, model, test_dataloader, args, mode=\"test\"\n",
    "    )\n",
    "    print(f\"===== Best validation Accuracy: {max(valid_accs):.3f} =====>\")\n",
    "\n",
    "    # Save log if logdir provided\n",
    "    if args.logdir is not None:\n",
    "        print(f'Writing training logs to {args.logdir}...')\n",
    "        os.makedirs(args.logdir, exist_ok=True)\n",
    "        with open(os.path.join(args.logdir, 'results.json'), 'w') as f:\n",
    "            f.write(json.dumps(\n",
    "                {\n",
    "                    \"train_losses\": train_losses,\n",
    "                    \"valid_losses\": valid_losses,\n",
    "                    \"train_accs\": train_accs,\n",
    "                    \"valid_accs\": valid_accs,\n",
    "                    \"test_loss\": test_loss,\n",
    "                    \"test_acc\": test_acc\n",
    "                },\n",
    "                indent=4,\n",
    "            ))\n",
    "    \n",
    "        # Visualize\n",
    "        if args.visualize and args.model in ['resnet18', 'mlpmixer']:\n",
    "            model.visualize(args.logdir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ZyJPWO1ppcTx"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_108314/812909305.py:6: UserWarning: CUDA is not available, make that your environment is running on GPU (e.g. in the Notebook Settings in Google Colab). Forcing device=\"cpu\".\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_108314/812909305.py:14: UserWarning: You are about to run on CPU, and might run out of memory shortly. You can try setting batch_size=1 to reduce memory usage.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Build model MLP...\n",
      "Loading model config from ./model_configs/mlp/mlp_relu.json\n",
      "########## MLP CONFIG ################\n",
      "input_size:\t3072\n",
      "hidden_sizes:\t[1024, 512, 64, 64]\n",
      "num_classes:\t10\n",
      "activation:\trelu\n",
      "############################################\n",
      "Initialized MLP model with 3709194 total parameters, of which 3709194 are learnable.\n",
      "====== Epoch 0 ======>\n",
      "[TRAIN] Epoch: 0, Iter: 0, Loss: 860876.31250\n",
      "[TRAIN] Epoch: 0, Iter: 80, Loss: 121053.90625\n",
      "[TRAIN] Epoch: 0, Iter: 160, Loss: 73105.87500\n",
      "[TRAIN] Epoch: 0, Iter: 240, Loss: 22707.79492\n",
      "[TRAIN] Epoch: 0, Iter: 320, Loss: 4666.03369\n",
      "== [TRAIN] Epoch: 0, Accuracy: 0.172 ==>\n",
      "[VAL] Epoch: 0, Iter: 0, Loss: 5203.04492\n",
      "=== [VAL] Epoch: 0, Iter: 39, Accuracy: 0.102 ===>\n",
      "====== Epoch 1 ======>\n",
      "[TRAIN] Epoch: 1, Iter: 0, Loss: 1527.32922\n",
      "[TRAIN] Epoch: 1, Iter: 80, Loss: 1741.62244\n",
      "[TRAIN] Epoch: 1, Iter: 160, Loss: 63.06256\n",
      "[TRAIN] Epoch: 1, Iter: 240, Loss: 166.84096\n",
      "[TRAIN] Epoch: 1, Iter: 320, Loss: 27.07744\n",
      "== [TRAIN] Epoch: 1, Accuracy: 0.106 ==>\n",
      "[VAL] Epoch: 1, Iter: 0, Loss: 724.65918\n",
      "=== [VAL] Epoch: 1, Iter: 39, Accuracy: 0.107 ===>\n",
      "====== Epoch 2 ======>\n",
      "[TRAIN] Epoch: 2, Iter: 0, Loss: 583.37323\n",
      "[TRAIN] Epoch: 2, Iter: 80, Loss: 531.16168\n",
      "[TRAIN] Epoch: 2, Iter: 160, Loss: 4.68196\n",
      "[TRAIN] Epoch: 2, Iter: 240, Loss: 105.28747\n",
      "[TRAIN] Epoch: 2, Iter: 320, Loss: 449.70517\n",
      "== [TRAIN] Epoch: 2, Accuracy: 0.102 ==>\n",
      "[VAL] Epoch: 2, Iter: 0, Loss: 68.39645\n",
      "=== [VAL] Epoch: 2, Iter: 39, Accuracy: 0.096 ===>\n",
      "====== Epoch 3 ======>\n",
      "[TRAIN] Epoch: 3, Iter: 0, Loss: 2.30391\n",
      "[TRAIN] Epoch: 3, Iter: 80, Loss: 2.30221\n",
      "[TRAIN] Epoch: 3, Iter: 160, Loss: 2.28431\n",
      "[TRAIN] Epoch: 3, Iter: 240, Loss: 96.27339\n",
      "[TRAIN] Epoch: 3, Iter: 320, Loss: 73.35696\n",
      "== [TRAIN] Epoch: 3, Accuracy: 0.097 ==>\n",
      "[VAL] Epoch: 3, Iter: 0, Loss: 2.30460\n",
      "=== [VAL] Epoch: 3, Iter: 39, Accuracy: 0.096 ===>\n",
      "====== Epoch 4 ======>\n",
      "[TRAIN] Epoch: 4, Iter: 0, Loss: 2.30267\n",
      "[TRAIN] Epoch: 4, Iter: 80, Loss: 97.43722\n",
      "[TRAIN] Epoch: 4, Iter: 160, Loss: 2.30354\n",
      "[TRAIN] Epoch: 4, Iter: 240, Loss: 2.30001\n",
      "[TRAIN] Epoch: 4, Iter: 320, Loss: 95.85760\n",
      "== [TRAIN] Epoch: 4, Accuracy: 0.101 ==>\n",
      "[VAL] Epoch: 4, Iter: 0, Loss: 2.30532\n",
      "=== [VAL] Epoch: 4, Iter: 39, Accuracy: 0.096 ===>\n",
      "====== Epoch 5 ======>\n",
      "[TRAIN] Epoch: 5, Iter: 0, Loss: 2.28625\n",
      "[TRAIN] Epoch: 5, Iter: 80, Loss: 2.30416\n",
      "[TRAIN] Epoch: 5, Iter: 160, Loss: 2.30331\n",
      "[TRAIN] Epoch: 5, Iter: 240, Loss: 2.30209\n",
      "[TRAIN] Epoch: 5, Iter: 320, Loss: 13.52055\n",
      "== [TRAIN] Epoch: 5, Accuracy: 0.101 ==>\n",
      "[VAL] Epoch: 5, Iter: 0, Loss: 2.30397\n",
      "=== [VAL] Epoch: 5, Iter: 39, Accuracy: 0.104 ===>\n",
      "====== Epoch 6 ======>\n",
      "[TRAIN] Epoch: 6, Iter: 0, Loss: 2.30238\n",
      "[TRAIN] Epoch: 6, Iter: 80, Loss: 169.27257\n",
      "[TRAIN] Epoch: 6, Iter: 160, Loss: 2.30406\n",
      "[TRAIN] Epoch: 6, Iter: 240, Loss: 2.30245\n",
      "[TRAIN] Epoch: 6, Iter: 320, Loss: 2.30458\n",
      "== [TRAIN] Epoch: 6, Accuracy: 0.101 ==>\n",
      "[VAL] Epoch: 6, Iter: 0, Loss: 2.30059\n",
      "=== [VAL] Epoch: 6, Iter: 39, Accuracy: 0.103 ===>\n",
      "====== Epoch 7 ======>\n",
      "[TRAIN] Epoch: 7, Iter: 0, Loss: 2.30216\n",
      "[TRAIN] Epoch: 7, Iter: 80, Loss: 2.30013\n",
      "[TRAIN] Epoch: 7, Iter: 160, Loss: 2.30272\n",
      "[TRAIN] Epoch: 7, Iter: 240, Loss: 2.30482\n",
      "[TRAIN] Epoch: 7, Iter: 320, Loss: 2.55546\n",
      "== [TRAIN] Epoch: 7, Accuracy: 0.100 ==>\n",
      "[VAL] Epoch: 7, Iter: 0, Loss: 2.30467\n",
      "=== [VAL] Epoch: 7, Iter: 39, Accuracy: 0.105 ===>\n",
      "====== Epoch 8 ======>\n",
      "[TRAIN] Epoch: 8, Iter: 0, Loss: 46.20964\n",
      "[TRAIN] Epoch: 8, Iter: 80, Loss: 2.30322\n",
      "[TRAIN] Epoch: 8, Iter: 160, Loss: 2.30443\n",
      "[TRAIN] Epoch: 8, Iter: 240, Loss: 74.75100\n",
      "[TRAIN] Epoch: 8, Iter: 320, Loss: 2.30300\n",
      "== [TRAIN] Epoch: 8, Accuracy: 0.098 ==>\n",
      "[VAL] Epoch: 8, Iter: 0, Loss: 2.30394\n",
      "=== [VAL] Epoch: 8, Iter: 39, Accuracy: 0.105 ===>\n",
      "====== Epoch 9 ======>\n",
      "[TRAIN] Epoch: 9, Iter: 0, Loss: 136.29317\n",
      "[TRAIN] Epoch: 9, Iter: 80, Loss: 2.30144\n",
      "[TRAIN] Epoch: 9, Iter: 160, Loss: 2.30244\n",
      "[TRAIN] Epoch: 9, Iter: 240, Loss: 2.30314\n",
      "[TRAIN] Epoch: 9, Iter: 320, Loss: 2.30132\n",
      "== [TRAIN] Epoch: 9, Accuracy: 0.100 ==>\n",
      "[VAL] Epoch: 9, Iter: 0, Loss: 2.30067\n",
      "=== [VAL] Epoch: 9, Iter: 39, Accuracy: 0.105 ===>\n",
      "====== Epoch 10 ======>\n",
      "[TRAIN] Epoch: 10, Iter: 0, Loss: 2.30389\n",
      "[TRAIN] Epoch: 10, Iter: 80, Loss: 2.30396\n",
      "[TRAIN] Epoch: 10, Iter: 160, Loss: 2.30188\n",
      "[TRAIN] Epoch: 10, Iter: 240, Loss: 2.30002\n",
      "[TRAIN] Epoch: 10, Iter: 320, Loss: 2.30341\n",
      "== [TRAIN] Epoch: 10, Accuracy: 0.098 ==>\n",
      "[VAL] Epoch: 10, Iter: 0, Loss: 2.30214\n",
      "=== [VAL] Epoch: 10, Iter: 39, Accuracy: 0.104 ===>\n",
      "====== Epoch 11 ======>\n",
      "[TRAIN] Epoch: 11, Iter: 0, Loss: 2.30248\n",
      "[TRAIN] Epoch: 11, Iter: 80, Loss: 2.30320\n",
      "[TRAIN] Epoch: 11, Iter: 160, Loss: 2.30179\n",
      "[TRAIN] Epoch: 11, Iter: 240, Loss: 2.30347\n",
      "[TRAIN] Epoch: 11, Iter: 320, Loss: 2.30345\n",
      "== [TRAIN] Epoch: 11, Accuracy: 0.100 ==>\n",
      "[VAL] Epoch: 11, Iter: 0, Loss: 2.30172\n",
      "=== [VAL] Epoch: 11, Iter: 39, Accuracy: 0.104 ===>\n",
      "====== Epoch 12 ======>\n",
      "[TRAIN] Epoch: 12, Iter: 0, Loss: 2.30278\n",
      "[TRAIN] Epoch: 12, Iter: 80, Loss: 2.30213\n",
      "[TRAIN] Epoch: 12, Iter: 160, Loss: 33.62466\n",
      "[TRAIN] Epoch: 12, Iter: 240, Loss: 2.30222\n",
      "[TRAIN] Epoch: 12, Iter: 320, Loss: 2.30325\n",
      "== [TRAIN] Epoch: 12, Accuracy: 0.100 ==>\n",
      "[VAL] Epoch: 12, Iter: 0, Loss: 2.30292\n",
      "=== [VAL] Epoch: 12, Iter: 39, Accuracy: 0.096 ===>\n",
      "====== Epoch 13 ======>\n",
      "[TRAIN] Epoch: 13, Iter: 0, Loss: 2.30208\n",
      "[TRAIN] Epoch: 13, Iter: 80, Loss: 2.30239\n",
      "[TRAIN] Epoch: 13, Iter: 160, Loss: 2.30424\n",
      "[TRAIN] Epoch: 13, Iter: 240, Loss: 2.30263\n",
      "[TRAIN] Epoch: 13, Iter: 320, Loss: 2.30254\n",
      "== [TRAIN] Epoch: 13, Accuracy: 0.101 ==>\n",
      "[VAL] Epoch: 13, Iter: 0, Loss: 2.30146\n",
      "=== [VAL] Epoch: 13, Iter: 39, Accuracy: 0.104 ===>\n",
      "====== Epoch 14 ======>\n",
      "[TRAIN] Epoch: 14, Iter: 0, Loss: 2.30130\n",
      "[TRAIN] Epoch: 14, Iter: 80, Loss: 2.30297\n",
      "[TRAIN] Epoch: 14, Iter: 160, Loss: 2.30342\n",
      "[TRAIN] Epoch: 14, Iter: 240, Loss: 2.30287\n",
      "[TRAIN] Epoch: 14, Iter: 320, Loss: 2.30115\n",
      "== [TRAIN] Epoch: 14, Accuracy: 0.098 ==>\n",
      "[VAL] Epoch: 14, Iter: 0, Loss: 2.30091\n",
      "=== [VAL] Epoch: 14, Iter: 39, Accuracy: 0.104 ===>\n",
      "[TEST] Epoch: 14, Iter: 0, Loss: 2.30248\n",
      "=== [TEST] Epoch: 14, Iter: 78, Accuracy: 0.101 ===>\n",
      "===== Best validation Accuracy: 0.107 =====>\n",
      "Writing training logs to exps/mlp/mlp_relu...\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Build model MLP...\n",
      "Loading model config from ./model_configs/mlp/mlp_sigmoid.json\n",
      "########## MLP CONFIG ################\n",
      "input_size:\t3072\n",
      "hidden_sizes:\t[1024, 512, 64, 64]\n",
      "num_classes:\t10\n",
      "activation:\tsigmoid\n",
      "############################################\n",
      "Initialized MLP model with 3709194 total parameters, of which 3709194 are learnable.\n",
      "====== Epoch 0 ======>\n",
      "[TRAIN] Epoch: 0, Iter: 0, Loss: 6.69644\n",
      "[TRAIN] Epoch: 0, Iter: 80, Loss: 2.44127\n",
      "[TRAIN] Epoch: 0, Iter: 160, Loss: 2.28531\n",
      "[TRAIN] Epoch: 0, Iter: 240, Loss: 2.22314\n",
      "[TRAIN] Epoch: 0, Iter: 320, Loss: 2.14684\n",
      "== [TRAIN] Epoch: 0, Accuracy: 0.173 ==>\n",
      "[VAL] Epoch: 0, Iter: 0, Loss: 2.14434\n",
      "=== [VAL] Epoch: 0, Iter: 39, Accuracy: 0.223 ===>\n",
      "====== Epoch 1 ======>\n",
      "[TRAIN] Epoch: 1, Iter: 0, Loss: 2.07825\n",
      "[TRAIN] Epoch: 1, Iter: 80, Loss: 2.05269\n",
      "[TRAIN] Epoch: 1, Iter: 160, Loss: 2.11881\n",
      "[TRAIN] Epoch: 1, Iter: 240, Loss: 2.15988\n",
      "[TRAIN] Epoch: 1, Iter: 320, Loss: 1.97476\n",
      "== [TRAIN] Epoch: 1, Accuracy: 0.251 ==>\n",
      "[VAL] Epoch: 1, Iter: 0, Loss: 1.99118\n",
      "=== [VAL] Epoch: 1, Iter: 39, Accuracy: 0.274 ===>\n",
      "====== Epoch 2 ======>\n",
      "[TRAIN] Epoch: 2, Iter: 0, Loss: 2.07169\n",
      "[TRAIN] Epoch: 2, Iter: 80, Loss: 1.96803\n",
      "[TRAIN] Epoch: 2, Iter: 160, Loss: 2.10816\n",
      "[TRAIN] Epoch: 2, Iter: 240, Loss: 1.96348\n",
      "[TRAIN] Epoch: 2, Iter: 320, Loss: 2.15021\n",
      "== [TRAIN] Epoch: 2, Accuracy: 0.285 ==>\n",
      "[VAL] Epoch: 2, Iter: 0, Loss: 2.04300\n",
      "=== [VAL] Epoch: 2, Iter: 39, Accuracy: 0.300 ===>\n",
      "====== Epoch 3 ======>\n",
      "[TRAIN] Epoch: 3, Iter: 0, Loss: 1.87821\n",
      "[TRAIN] Epoch: 3, Iter: 80, Loss: 1.88357\n",
      "[TRAIN] Epoch: 3, Iter: 160, Loss: 2.02496\n",
      "[TRAIN] Epoch: 3, Iter: 240, Loss: 1.94173\n",
      "[TRAIN] Epoch: 3, Iter: 320, Loss: 1.84858\n",
      "== [TRAIN] Epoch: 3, Accuracy: 0.303 ==>\n",
      "[VAL] Epoch: 3, Iter: 0, Loss: 2.04587\n",
      "=== [VAL] Epoch: 3, Iter: 39, Accuracy: 0.313 ===>\n",
      "====== Epoch 4 ======>\n",
      "[TRAIN] Epoch: 4, Iter: 0, Loss: 1.92758\n",
      "[TRAIN] Epoch: 4, Iter: 80, Loss: 1.96914\n",
      "[TRAIN] Epoch: 4, Iter: 160, Loss: 1.99505\n",
      "[TRAIN] Epoch: 4, Iter: 240, Loss: 1.92050\n",
      "[TRAIN] Epoch: 4, Iter: 320, Loss: 1.84219\n",
      "== [TRAIN] Epoch: 4, Accuracy: 0.314 ==>\n",
      "[VAL] Epoch: 4, Iter: 0, Loss: 1.97659\n",
      "=== [VAL] Epoch: 4, Iter: 39, Accuracy: 0.334 ===>\n",
      "====== Epoch 5 ======>\n",
      "[TRAIN] Epoch: 5, Iter: 0, Loss: 1.84106\n",
      "[TRAIN] Epoch: 5, Iter: 80, Loss: 1.76975\n",
      "[TRAIN] Epoch: 5, Iter: 160, Loss: 1.90777\n",
      "[TRAIN] Epoch: 5, Iter: 240, Loss: 1.80492\n",
      "[TRAIN] Epoch: 5, Iter: 320, Loss: 1.80976\n",
      "== [TRAIN] Epoch: 5, Accuracy: 0.328 ==>\n",
      "[VAL] Epoch: 5, Iter: 0, Loss: 1.99183\n",
      "=== [VAL] Epoch: 5, Iter: 39, Accuracy: 0.335 ===>\n",
      "====== Epoch 6 ======>\n",
      "[TRAIN] Epoch: 6, Iter: 0, Loss: 1.86291\n",
      "[TRAIN] Epoch: 6, Iter: 80, Loss: 1.81318\n",
      "[TRAIN] Epoch: 6, Iter: 160, Loss: 1.85300\n",
      "[TRAIN] Epoch: 6, Iter: 240, Loss: 1.82863\n",
      "[TRAIN] Epoch: 6, Iter: 320, Loss: 1.81873\n",
      "== [TRAIN] Epoch: 6, Accuracy: 0.335 ==>\n",
      "[VAL] Epoch: 6, Iter: 0, Loss: 1.96759\n",
      "=== [VAL] Epoch: 6, Iter: 39, Accuracy: 0.331 ===>\n",
      "====== Epoch 7 ======>\n",
      "[TRAIN] Epoch: 7, Iter: 0, Loss: 1.89659\n",
      "[TRAIN] Epoch: 7, Iter: 80, Loss: 1.90885\n",
      "[TRAIN] Epoch: 7, Iter: 160, Loss: 1.77985\n",
      "[TRAIN] Epoch: 7, Iter: 240, Loss: 1.86680\n",
      "[TRAIN] Epoch: 7, Iter: 320, Loss: 1.77685\n",
      "== [TRAIN] Epoch: 7, Accuracy: 0.339 ==>\n",
      "[VAL] Epoch: 7, Iter: 0, Loss: 1.96770\n",
      "=== [VAL] Epoch: 7, Iter: 39, Accuracy: 0.343 ===>\n",
      "====== Epoch 8 ======>\n",
      "[TRAIN] Epoch: 8, Iter: 0, Loss: 1.77232\n",
      "[TRAIN] Epoch: 8, Iter: 80, Loss: 1.90943\n",
      "[TRAIN] Epoch: 8, Iter: 160, Loss: 1.80036\n",
      "[TRAIN] Epoch: 8, Iter: 240, Loss: 1.92219\n",
      "[TRAIN] Epoch: 8, Iter: 320, Loss: 1.79632\n",
      "== [TRAIN] Epoch: 8, Accuracy: 0.344 ==>\n",
      "[VAL] Epoch: 8, Iter: 0, Loss: 1.94057\n",
      "=== [VAL] Epoch: 8, Iter: 39, Accuracy: 0.348 ===>\n",
      "====== Epoch 9 ======>\n",
      "[TRAIN] Epoch: 9, Iter: 0, Loss: 1.86804\n",
      "[TRAIN] Epoch: 9, Iter: 80, Loss: 1.73920\n",
      "[TRAIN] Epoch: 9, Iter: 160, Loss: 1.84829\n",
      "[TRAIN] Epoch: 9, Iter: 240, Loss: 1.85572\n",
      "[TRAIN] Epoch: 9, Iter: 320, Loss: 1.87319\n",
      "== [TRAIN] Epoch: 9, Accuracy: 0.348 ==>\n",
      "[VAL] Epoch: 9, Iter: 0, Loss: 1.87618\n",
      "=== [VAL] Epoch: 9, Iter: 39, Accuracy: 0.355 ===>\n",
      "====== Epoch 10 ======>\n",
      "[TRAIN] Epoch: 10, Iter: 0, Loss: 1.87153\n",
      "[TRAIN] Epoch: 10, Iter: 80, Loss: 1.74767\n",
      "[TRAIN] Epoch: 10, Iter: 160, Loss: 1.77036\n",
      "[TRAIN] Epoch: 10, Iter: 240, Loss: 1.76459\n",
      "[TRAIN] Epoch: 10, Iter: 320, Loss: 1.88355\n",
      "== [TRAIN] Epoch: 10, Accuracy: 0.353 ==>\n",
      "[VAL] Epoch: 10, Iter: 0, Loss: 1.87877\n",
      "=== [VAL] Epoch: 10, Iter: 39, Accuracy: 0.354 ===>\n",
      "====== Epoch 11 ======>\n",
      "[TRAIN] Epoch: 11, Iter: 0, Loss: 1.68209\n",
      "[TRAIN] Epoch: 11, Iter: 80, Loss: 1.86954\n",
      "[TRAIN] Epoch: 11, Iter: 160, Loss: 1.72758\n",
      "[TRAIN] Epoch: 11, Iter: 240, Loss: 1.94091\n",
      "[TRAIN] Epoch: 11, Iter: 320, Loss: 1.69859\n",
      "== [TRAIN] Epoch: 11, Accuracy: 0.354 ==>\n",
      "[VAL] Epoch: 11, Iter: 0, Loss: 1.84229\n",
      "=== [VAL] Epoch: 11, Iter: 39, Accuracy: 0.372 ===>\n",
      "====== Epoch 12 ======>\n",
      "[TRAIN] Epoch: 12, Iter: 0, Loss: 1.67503\n",
      "[TRAIN] Epoch: 12, Iter: 80, Loss: 1.77410\n",
      "[TRAIN] Epoch: 12, Iter: 160, Loss: 1.85993\n",
      "[TRAIN] Epoch: 12, Iter: 240, Loss: 1.90537\n",
      "[TRAIN] Epoch: 12, Iter: 320, Loss: 1.73172\n",
      "== [TRAIN] Epoch: 12, Accuracy: 0.361 ==>\n",
      "[VAL] Epoch: 12, Iter: 0, Loss: 1.83707\n",
      "=== [VAL] Epoch: 12, Iter: 39, Accuracy: 0.366 ===>\n",
      "====== Epoch 13 ======>\n",
      "[TRAIN] Epoch: 13, Iter: 0, Loss: 1.70467\n",
      "[TRAIN] Epoch: 13, Iter: 80, Loss: 1.82779\n",
      "[TRAIN] Epoch: 13, Iter: 160, Loss: 1.83482\n",
      "[TRAIN] Epoch: 13, Iter: 240, Loss: 1.74741\n",
      "[TRAIN] Epoch: 13, Iter: 320, Loss: 1.72072\n",
      "== [TRAIN] Epoch: 13, Accuracy: 0.363 ==>\n",
      "[VAL] Epoch: 13, Iter: 0, Loss: 1.84669\n",
      "=== [VAL] Epoch: 13, Iter: 39, Accuracy: 0.372 ===>\n",
      "====== Epoch 14 ======>\n",
      "[TRAIN] Epoch: 14, Iter: 0, Loss: 1.73169\n",
      "[TRAIN] Epoch: 14, Iter: 80, Loss: 1.79373\n",
      "[TRAIN] Epoch: 14, Iter: 160, Loss: 1.78059\n",
      "[TRAIN] Epoch: 14, Iter: 240, Loss: 1.81837\n",
      "[TRAIN] Epoch: 14, Iter: 320, Loss: 1.85752\n",
      "== [TRAIN] Epoch: 14, Accuracy: 0.370 ==>\n",
      "[VAL] Epoch: 14, Iter: 0, Loss: 1.82901\n",
      "=== [VAL] Epoch: 14, Iter: 39, Accuracy: 0.373 ===>\n",
      "[TEST] Epoch: 14, Iter: 0, Loss: 1.66662\n",
      "=== [TEST] Epoch: 14, Iter: 78, Accuracy: 0.371 ===>\n",
      "===== Best validation Accuracy: 0.373 =====>\n",
      "Writing training logs to exps/mlp/mlp_sigmoid...\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Build model MLP...\n",
      "Loading model config from ./model_configs/mlp/mlp_tanh.json\n",
      "########## MLP CONFIG ################\n",
      "input_size:\t3072\n",
      "hidden_sizes:\t[1024, 512, 64, 64]\n",
      "num_classes:\t10\n",
      "activation:\ttanh\n",
      "############################################\n",
      "Initialized MLP model with 3709194 total parameters, of which 3709194 are learnable.\n",
      "====== Epoch 0 ======>\n",
      "[TRAIN] Epoch: 0, Iter: 0, Loss: 12.01356\n",
      "[TRAIN] Epoch: 0, Iter: 80, Loss: 9.65080\n",
      "[TRAIN] Epoch: 0, Iter: 160, Loss: 9.19007\n",
      "[TRAIN] Epoch: 0, Iter: 240, Loss: 8.13797\n",
      "[TRAIN] Epoch: 0, Iter: 320, Loss: 7.02007\n",
      "== [TRAIN] Epoch: 0, Accuracy: 0.130 ==>\n",
      "[VAL] Epoch: 0, Iter: 0, Loss: 8.24478\n",
      "=== [VAL] Epoch: 0, Iter: 39, Accuracy: 0.145 ===>\n",
      "====== Epoch 1 ======>\n",
      "[TRAIN] Epoch: 1, Iter: 0, Loss: 6.87455\n",
      "[TRAIN] Epoch: 1, Iter: 80, Loss: 6.66124\n",
      "[TRAIN] Epoch: 1, Iter: 160, Loss: 6.54756\n",
      "[TRAIN] Epoch: 1, Iter: 240, Loss: 5.67858\n",
      "[TRAIN] Epoch: 1, Iter: 320, Loss: 5.13331\n",
      "== [TRAIN] Epoch: 1, Accuracy: 0.151 ==>\n",
      "[VAL] Epoch: 1, Iter: 0, Loss: 5.85286\n",
      "=== [VAL] Epoch: 1, Iter: 39, Accuracy: 0.160 ===>\n",
      "====== Epoch 2 ======>\n",
      "[TRAIN] Epoch: 2, Iter: 0, Loss: 5.78538\n",
      "[TRAIN] Epoch: 2, Iter: 80, Loss: 4.99565\n",
      "[TRAIN] Epoch: 2, Iter: 160, Loss: 5.13180\n",
      "[TRAIN] Epoch: 2, Iter: 240, Loss: 4.92279\n",
      "[TRAIN] Epoch: 2, Iter: 320, Loss: 4.51738\n",
      "== [TRAIN] Epoch: 2, Accuracy: 0.158 ==>\n",
      "[VAL] Epoch: 2, Iter: 0, Loss: 5.24760\n",
      "=== [VAL] Epoch: 2, Iter: 39, Accuracy: 0.161 ===>\n",
      "====== Epoch 3 ======>\n",
      "[TRAIN] Epoch: 3, Iter: 0, Loss: 4.56911\n",
      "[TRAIN] Epoch: 3, Iter: 80, Loss: 4.48056\n",
      "[TRAIN] Epoch: 3, Iter: 160, Loss: 4.43606\n",
      "[TRAIN] Epoch: 3, Iter: 240, Loss: 4.58107\n",
      "[TRAIN] Epoch: 3, Iter: 320, Loss: 3.91385\n",
      "== [TRAIN] Epoch: 3, Accuracy: 0.165 ==>\n",
      "[VAL] Epoch: 3, Iter: 0, Loss: 4.16826\n",
      "=== [VAL] Epoch: 3, Iter: 39, Accuracy: 0.164 ===>\n",
      "====== Epoch 4 ======>\n",
      "[TRAIN] Epoch: 4, Iter: 0, Loss: 3.43866\n",
      "[TRAIN] Epoch: 4, Iter: 80, Loss: 3.87776\n",
      "[TRAIN] Epoch: 4, Iter: 160, Loss: 3.60124\n",
      "[TRAIN] Epoch: 4, Iter: 240, Loss: 3.36573\n",
      "[TRAIN] Epoch: 4, Iter: 320, Loss: 3.64387\n",
      "== [TRAIN] Epoch: 4, Accuracy: 0.169 ==>\n",
      "[VAL] Epoch: 4, Iter: 0, Loss: 3.64072\n",
      "=== [VAL] Epoch: 4, Iter: 39, Accuracy: 0.173 ===>\n",
      "====== Epoch 5 ======>\n",
      "[TRAIN] Epoch: 5, Iter: 0, Loss: 3.38076\n",
      "[TRAIN] Epoch: 5, Iter: 80, Loss: 3.22625\n",
      "[TRAIN] Epoch: 5, Iter: 160, Loss: 3.50642\n",
      "[TRAIN] Epoch: 5, Iter: 240, Loss: 2.98183\n",
      "[TRAIN] Epoch: 5, Iter: 320, Loss: 3.27073\n",
      "== [TRAIN] Epoch: 5, Accuracy: 0.173 ==>\n",
      "[VAL] Epoch: 5, Iter: 0, Loss: 3.30220\n",
      "=== [VAL] Epoch: 5, Iter: 39, Accuracy: 0.182 ===>\n",
      "====== Epoch 6 ======>\n",
      "[TRAIN] Epoch: 6, Iter: 0, Loss: 2.88941\n",
      "[TRAIN] Epoch: 6, Iter: 80, Loss: 2.68598\n",
      "[TRAIN] Epoch: 6, Iter: 160, Loss: 2.84957\n",
      "[TRAIN] Epoch: 6, Iter: 240, Loss: 2.67040\n",
      "[TRAIN] Epoch: 6, Iter: 320, Loss: 2.76892\n",
      "== [TRAIN] Epoch: 6, Accuracy: 0.181 ==>\n",
      "[VAL] Epoch: 6, Iter: 0, Loss: 3.06252\n",
      "=== [VAL] Epoch: 6, Iter: 39, Accuracy: 0.178 ===>\n",
      "====== Epoch 7 ======>\n",
      "[TRAIN] Epoch: 7, Iter: 0, Loss: 2.65896\n",
      "[TRAIN] Epoch: 7, Iter: 80, Loss: 2.47409\n",
      "[TRAIN] Epoch: 7, Iter: 160, Loss: 2.59236\n",
      "[TRAIN] Epoch: 7, Iter: 240, Loss: 2.52975\n",
      "[TRAIN] Epoch: 7, Iter: 320, Loss: 2.45720\n",
      "== [TRAIN] Epoch: 7, Accuracy: 0.188 ==>\n",
      "[VAL] Epoch: 7, Iter: 0, Loss: 2.80234\n",
      "=== [VAL] Epoch: 7, Iter: 39, Accuracy: 0.189 ===>\n",
      "====== Epoch 8 ======>\n",
      "[TRAIN] Epoch: 8, Iter: 0, Loss: 2.65421\n",
      "[TRAIN] Epoch: 8, Iter: 80, Loss: 2.53625\n",
      "[TRAIN] Epoch: 8, Iter: 160, Loss: 2.43905\n",
      "[TRAIN] Epoch: 8, Iter: 240, Loss: 2.51480\n",
      "[TRAIN] Epoch: 8, Iter: 320, Loss: 2.60183\n",
      "== [TRAIN] Epoch: 8, Accuracy: 0.195 ==>\n",
      "[VAL] Epoch: 8, Iter: 0, Loss: 2.60546\n",
      "=== [VAL] Epoch: 8, Iter: 39, Accuracy: 0.194 ===>\n",
      "====== Epoch 9 ======>\n",
      "[TRAIN] Epoch: 9, Iter: 0, Loss: 2.34906\n",
      "[TRAIN] Epoch: 9, Iter: 80, Loss: 2.17041\n",
      "[TRAIN] Epoch: 9, Iter: 160, Loss: 2.37608\n",
      "[TRAIN] Epoch: 9, Iter: 240, Loss: 2.24955\n",
      "[TRAIN] Epoch: 9, Iter: 320, Loss: 2.40054\n",
      "== [TRAIN] Epoch: 9, Accuracy: 0.206 ==>\n",
      "[VAL] Epoch: 9, Iter: 0, Loss: 2.31829\n",
      "=== [VAL] Epoch: 9, Iter: 39, Accuracy: 0.209 ===>\n",
      "====== Epoch 10 ======>\n",
      "[TRAIN] Epoch: 10, Iter: 0, Loss: 2.35355\n",
      "[TRAIN] Epoch: 10, Iter: 80, Loss: 2.24295\n",
      "[TRAIN] Epoch: 10, Iter: 160, Loss: 2.16489\n",
      "[TRAIN] Epoch: 10, Iter: 240, Loss: 2.24638\n",
      "[TRAIN] Epoch: 10, Iter: 320, Loss: 2.15806\n",
      "== [TRAIN] Epoch: 10, Accuracy: 0.213 ==>\n",
      "[VAL] Epoch: 10, Iter: 0, Loss: 2.18451\n",
      "=== [VAL] Epoch: 10, Iter: 39, Accuracy: 0.211 ===>\n",
      "====== Epoch 11 ======>\n",
      "[TRAIN] Epoch: 11, Iter: 0, Loss: 2.15663\n",
      "[TRAIN] Epoch: 11, Iter: 80, Loss: 2.27993\n",
      "[TRAIN] Epoch: 11, Iter: 160, Loss: 2.25359\n",
      "[TRAIN] Epoch: 11, Iter: 240, Loss: 2.26427\n",
      "[TRAIN] Epoch: 11, Iter: 320, Loss: 2.20890\n",
      "== [TRAIN] Epoch: 11, Accuracy: 0.219 ==>\n",
      "[VAL] Epoch: 11, Iter: 0, Loss: 2.14724\n",
      "=== [VAL] Epoch: 11, Iter: 39, Accuracy: 0.230 ===>\n",
      "====== Epoch 12 ======>\n",
      "[TRAIN] Epoch: 12, Iter: 0, Loss: 2.14758\n",
      "[TRAIN] Epoch: 12, Iter: 80, Loss: 2.17968\n",
      "[TRAIN] Epoch: 12, Iter: 160, Loss: 2.17257\n",
      "[TRAIN] Epoch: 12, Iter: 240, Loss: 2.28039\n",
      "[TRAIN] Epoch: 12, Iter: 320, Loss: 2.12231\n",
      "== [TRAIN] Epoch: 12, Accuracy: 0.227 ==>\n",
      "[VAL] Epoch: 12, Iter: 0, Loss: 2.17374\n",
      "=== [VAL] Epoch: 12, Iter: 39, Accuracy: 0.230 ===>\n",
      "====== Epoch 13 ======>\n",
      "[TRAIN] Epoch: 13, Iter: 0, Loss: 2.18905\n",
      "[TRAIN] Epoch: 13, Iter: 80, Loss: 2.11122\n",
      "[TRAIN] Epoch: 13, Iter: 160, Loss: 2.13365\n",
      "[TRAIN] Epoch: 13, Iter: 240, Loss: 2.08386\n",
      "[TRAIN] Epoch: 13, Iter: 320, Loss: 2.03104\n",
      "== [TRAIN] Epoch: 13, Accuracy: 0.236 ==>\n",
      "[VAL] Epoch: 13, Iter: 0, Loss: 2.08994\n",
      "=== [VAL] Epoch: 13, Iter: 39, Accuracy: 0.227 ===>\n",
      "====== Epoch 14 ======>\n",
      "[TRAIN] Epoch: 14, Iter: 0, Loss: 2.15561\n",
      "[TRAIN] Epoch: 14, Iter: 80, Loss: 2.01747\n",
      "[TRAIN] Epoch: 14, Iter: 160, Loss: 2.05747\n",
      "[TRAIN] Epoch: 14, Iter: 240, Loss: 2.09203\n",
      "[TRAIN] Epoch: 14, Iter: 320, Loss: 2.16863\n",
      "== [TRAIN] Epoch: 14, Accuracy: 0.241 ==>\n",
      "[VAL] Epoch: 14, Iter: 0, Loss: 2.04739\n",
      "=== [VAL] Epoch: 14, Iter: 39, Accuracy: 0.235 ===>\n",
      "[TEST] Epoch: 14, Iter: 0, Loss: 2.00109\n",
      "=== [TEST] Epoch: 14, Iter: 78, Accuracy: 0.252 ===>\n",
      "===== Best validation Accuracy: 0.235 =====>\n",
      "Writing training logs to exps/mlp/mlp_tanh...\n"
     ]
    }
   ],
   "source": [
    "model = 'mlp'\n",
    "for nl in ['relu', 'sigmoid', 'tanh']:\n",
    "    config = Arguments(model=model,\n",
    "                       model_config=f'./model_configs/{model}/{model}_{nl}.json',\n",
    "                       epochs=15,\n",
    "                       logdir=f'exps/{model}/{model}_{nl}')\n",
    "    main_entry(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m generate_plots\n\u001b[0;32m----> 3\u001b[0m list_of_dirs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexps/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m nl \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtanh\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m      5\u001b[0m legend_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReLU\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTanh\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      7\u001b[0m mlp_fig \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m'\u001b[39m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m generate_plots\n\u001b[0;32m----> 3\u001b[0m list_of_dirs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexps/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mmodel\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m nl \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtanh\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m      5\u001b[0m legend_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReLU\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTanh\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      7\u001b[0m mlp_fig \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m'\u001b[39m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from utils import generate_plots\n",
    "\n",
    "list_of_dirs = [f'exps/{model}/{model}_{nl}' for nl in ['relu', 'sigmoid', 'tanh']]\n",
    "\n",
    "legend_names = ['ReLU', 'Sigmoid', 'Tanh']\n",
    "\n",
    "mlp_fig = os.makedirs('results', exist_ok=True)\n",
    "\n",
    "generate_plots(list_of_dirs, legend_names, 'results')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©finition des taux d'apprentissage √† tester\n",
    "learning_rates = [0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "logdir = \"logs_resnet18\"\n",
    "os.makedirs(logdir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_lr(lr, logdir):\n",
    "    \"\"\"Entra√Æne ResNet18 avec un taux d'apprentissage donn√© en utilisant main_entry\"\"\"\n",
    "    args = Arguments()\n",
    "    args.lr = lr  # Modifier le taux d'apprentissage\n",
    "    args.model = 'resnet18'\n",
    "    args.optimizer = 'adam'\n",
    "    args.model_config = \"./model_configs/resnet18.json\"  # Correction du chemin\n",
    "    args.logdir = os.path.join(logdir, f\"lr_{lr}\")\n",
    "    os.makedirs(args.logdir, exist_ok=True)\n",
    "    \n",
    "    # Lancer l'entra√Ænement avec main_entry\n",
    "    main_entry(args)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_111463/812909305.py:6: UserWarning: CUDA is not available, make that your environment is running on GPU (e.g. in the Notebook Settings in Google Colab). Forcing device=\"cpu\".\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_111463/812909305.py:14: UserWarning: You are about to run on CPU, and might run out of memory shortly. You can try setting batch_size=1 to reduce memory usage.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Build model RESNET18...\n",
      "Loading model config from ./model_configs/resnet18.json\n",
      "########## RESNET18 CONFIG ################\n",
      "num_classes:\t10\n",
      "############################################\n",
      "Initialized RESNET18 model with 11173962 total parameters, of which 11173962 are learnable.\n",
      "====== Epoch 0 ======>\n",
      "[TRAIN] Epoch: 0, Iter: 0, Loss: 2.35367\n",
      "[TRAIN] Epoch: 0, Iter: 80, Loss: nan\n",
      "[TRAIN] Epoch: 0, Iter: 160, Loss: nan\n",
      "[TRAIN] Epoch: 0, Iter: 240, Loss: nan\n",
      "[TRAIN] Epoch: 0, Iter: 320, Loss: nan\n",
      "== [TRAIN] Epoch: 0, Accuracy: 0.100 ==>\n",
      "[VAL] Epoch: 0, Iter: 0, Loss: nan\n",
      "=== [VAL] Epoch: 0, Iter: 39, Accuracy: 0.103 ===>\n",
      "====== Epoch 1 ======>\n",
      "[TRAIN] Epoch: 1, Iter: 0, Loss: nan\n",
      "[TRAIN] Epoch: 1, Iter: 80, Loss: nan\n",
      "[TRAIN] Epoch: 1, Iter: 160, Loss: nan\n",
      "[TRAIN] Epoch: 1, Iter: 240, Loss: nan\n",
      "[TRAIN] Epoch: 1, Iter: 320, Loss: nan\n",
      "== [TRAIN] Epoch: 1, Accuracy: 0.100 ==>\n",
      "[VAL] Epoch: 1, Iter: 0, Loss: nan\n",
      "=== [VAL] Epoch: 1, Iter: 39, Accuracy: 0.103 ===>\n",
      "====== Epoch 2 ======>\n",
      "[TRAIN] Epoch: 2, Iter: 0, Loss: nan\n",
      "[TRAIN] Epoch: 2, Iter: 80, Loss: nan\n",
      "[TRAIN] Epoch: 2, Iter: 160, Loss: nan\n",
      "[TRAIN] Epoch: 2, Iter: 240, Loss: nan\n",
      "[TRAIN] Epoch: 2, Iter: 320, Loss: nan\n",
      "== [TRAIN] Epoch: 2, Accuracy: 0.100 ==>\n",
      "[VAL] Epoch: 2, Iter: 0, Loss: nan\n",
      "=== [VAL] Epoch: 2, Iter: 39, Accuracy: 0.103 ===>\n",
      "====== Epoch 3 ======>\n",
      "[TRAIN] Epoch: 3, Iter: 0, Loss: nan\n",
      "[TRAIN] Epoch: 3, Iter: 80, Loss: nan\n",
      "[TRAIN] Epoch: 3, Iter: 160, Loss: nan\n",
      "[TRAIN] Epoch: 3, Iter: 240, Loss: nan\n",
      "[TRAIN] Epoch: 3, Iter: 320, Loss: nan\n",
      "== [TRAIN] Epoch: 3, Accuracy: 0.100 ==>\n",
      "[VAL] Epoch: 3, Iter: 0, Loss: nan\n",
      "=== [VAL] Epoch: 3, Iter: 39, Accuracy: 0.103 ===>\n",
      "====== Epoch 4 ======>\n",
      "[TRAIN] Epoch: 4, Iter: 0, Loss: nan\n",
      "[TRAIN] Epoch: 4, Iter: 80, Loss: nan\n",
      "[TRAIN] Epoch: 4, Iter: 160, Loss: nan\n",
      "[TRAIN] Epoch: 4, Iter: 240, Loss: nan\n",
      "[TRAIN] Epoch: 4, Iter: 320, Loss: nan\n",
      "== [TRAIN] Epoch: 4, Accuracy: 0.100 ==>\n",
      "[VAL] Epoch: 4, Iter: 0, Loss: nan\n",
      "=== [VAL] Epoch: 4, Iter: 39, Accuracy: 0.103 ===>\n",
      "====== Epoch 5 ======>\n",
      "[TRAIN] Epoch: 5, Iter: 0, Loss: nan\n",
      "[TRAIN] Epoch: 5, Iter: 80, Loss: nan\n",
      "[TRAIN] Epoch: 5, Iter: 160, Loss: nan\n",
      "[TRAIN] Epoch: 5, Iter: 240, Loss: nan\n",
      "[TRAIN] Epoch: 5, Iter: 320, Loss: nan\n",
      "== [TRAIN] Epoch: 5, Accuracy: 0.100 ==>\n",
      "[VAL] Epoch: 5, Iter: 0, Loss: nan\n",
      "=== [VAL] Epoch: 5, Iter: 39, Accuracy: 0.103 ===>\n",
      "====== Epoch 6 ======>\n",
      "[TRAIN] Epoch: 6, Iter: 0, Loss: nan\n",
      "[TRAIN] Epoch: 6, Iter: 80, Loss: nan\n",
      "[TRAIN] Epoch: 6, Iter: 160, Loss: nan\n",
      "[TRAIN] Epoch: 6, Iter: 240, Loss: nan\n",
      "[TRAIN] Epoch: 6, Iter: 320, Loss: nan\n",
      "== [TRAIN] Epoch: 6, Accuracy: 0.100 ==>\n",
      "[VAL] Epoch: 6, Iter: 0, Loss: nan\n",
      "=== [VAL] Epoch: 6, Iter: 39, Accuracy: 0.103 ===>\n",
      "====== Epoch 7 ======>\n",
      "[TRAIN] Epoch: 7, Iter: 0, Loss: nan\n",
      "[TRAIN] Epoch: 7, Iter: 80, Loss: nan\n",
      "[TRAIN] Epoch: 7, Iter: 160, Loss: nan\n",
      "[TRAIN] Epoch: 7, Iter: 240, Loss: nan\n",
      "[TRAIN] Epoch: 7, Iter: 320, Loss: nan\n",
      "== [TRAIN] Epoch: 7, Accuracy: 0.100 ==>\n",
      "[VAL] Epoch: 7, Iter: 0, Loss: nan\n",
      "=== [VAL] Epoch: 7, Iter: 39, Accuracy: 0.103 ===>\n",
      "====== Epoch 8 ======>\n",
      "[TRAIN] Epoch: 8, Iter: 0, Loss: nan\n",
      "[TRAIN] Epoch: 8, Iter: 80, Loss: nan\n",
      "[TRAIN] Epoch: 8, Iter: 160, Loss: nan\n",
      "[TRAIN] Epoch: 8, Iter: 240, Loss: nan\n",
      "[TRAIN] Epoch: 8, Iter: 320, Loss: nan\n",
      "== [TRAIN] Epoch: 8, Accuracy: 0.100 ==>\n",
      "[VAL] Epoch: 8, Iter: 0, Loss: nan\n",
      "=== [VAL] Epoch: 8, Iter: 39, Accuracy: 0.103 ===>\n",
      "====== Epoch 9 ======>\n",
      "[TRAIN] Epoch: 9, Iter: 0, Loss: nan\n",
      "[TRAIN] Epoch: 9, Iter: 80, Loss: nan\n",
      "[TRAIN] Epoch: 9, Iter: 160, Loss: nan\n",
      "[TRAIN] Epoch: 9, Iter: 240, Loss: nan\n",
      "[TRAIN] Epoch: 9, Iter: 320, Loss: nan\n",
      "== [TRAIN] Epoch: 9, Accuracy: 0.100 ==>\n",
      "[VAL] Epoch: 9, Iter: 0, Loss: nan\n",
      "=== [VAL] Epoch: 9, Iter: 39, Accuracy: 0.103 ===>\n",
      "====== Epoch 10 ======>\n",
      "[TRAIN] Epoch: 10, Iter: 0, Loss: nan\n",
      "[TRAIN] Epoch: 10, Iter: 80, Loss: nan\n",
      "[TRAIN] Epoch: 10, Iter: 160, Loss: nan\n",
      "[TRAIN] Epoch: 10, Iter: 240, Loss: nan\n",
      "[TRAIN] Epoch: 10, Iter: 320, Loss: nan\n",
      "== [TRAIN] Epoch: 10, Accuracy: 0.100 ==>\n",
      "[VAL] Epoch: 10, Iter: 0, Loss: nan\n",
      "=== [VAL] Epoch: 10, Iter: 39, Accuracy: 0.103 ===>\n",
      "====== Epoch 11 ======>\n",
      "[TRAIN] Epoch: 11, Iter: 0, Loss: nan\n",
      "[TRAIN] Epoch: 11, Iter: 80, Loss: nan\n",
      "[TRAIN] Epoch: 11, Iter: 160, Loss: nan\n",
      "[TRAIN] Epoch: 11, Iter: 240, Loss: nan\n",
      "[TRAIN] Epoch: 11, Iter: 320, Loss: nan\n",
      "== [TRAIN] Epoch: 11, Accuracy: 0.100 ==>\n",
      "[VAL] Epoch: 11, Iter: 0, Loss: nan\n",
      "=== [VAL] Epoch: 11, Iter: 39, Accuracy: 0.103 ===>\n",
      "====== Epoch 12 ======>\n",
      "[TRAIN] Epoch: 12, Iter: 0, Loss: nan\n",
      "[TRAIN] Epoch: 12, Iter: 80, Loss: nan\n",
      "[TRAIN] Epoch: 12, Iter: 160, Loss: nan\n",
      "[TRAIN] Epoch: 12, Iter: 240, Loss: nan\n",
      "[TRAIN] Epoch: 12, Iter: 320, Loss: nan\n",
      "== [TRAIN] Epoch: 12, Accuracy: 0.100 ==>\n",
      "[VAL] Epoch: 12, Iter: 0, Loss: nan\n",
      "=== [VAL] Epoch: 12, Iter: 39, Accuracy: 0.103 ===>\n",
      "====== Epoch 13 ======>\n",
      "[TRAIN] Epoch: 13, Iter: 0, Loss: nan\n",
      "[TRAIN] Epoch: 13, Iter: 80, Loss: nan\n",
      "[TRAIN] Epoch: 13, Iter: 160, Loss: nan\n",
      "[TRAIN] Epoch: 13, Iter: 240, Loss: nan\n",
      "[TRAIN] Epoch: 13, Iter: 320, Loss: nan\n",
      "== [TRAIN] Epoch: 13, Accuracy: 0.100 ==>\n",
      "[VAL] Epoch: 13, Iter: 0, Loss: nan\n",
      "=== [VAL] Epoch: 13, Iter: 39, Accuracy: 0.103 ===>\n",
      "====== Epoch 14 ======>\n",
      "[TRAIN] Epoch: 14, Iter: 0, Loss: nan\n",
      "[TRAIN] Epoch: 14, Iter: 80, Loss: nan\n",
      "[TRAIN] Epoch: 14, Iter: 160, Loss: nan\n",
      "[TRAIN] Epoch: 14, Iter: 240, Loss: nan\n",
      "[TRAIN] Epoch: 14, Iter: 320, Loss: nan\n",
      "== [TRAIN] Epoch: 14, Accuracy: 0.100 ==>\n",
      "[VAL] Epoch: 14, Iter: 0, Loss: nan\n",
      "=== [VAL] Epoch: 14, Iter: 39, Accuracy: 0.103 ===>\n",
      "[TEST] Epoch: 14, Iter: 0, Loss: nan\n",
      "=== [TEST] Epoch: 14, Iter: 78, Accuracy: 0.100 ===>\n",
      "===== Best validation Accuracy: 0.103 =====>\n",
      "Writing training logs to logs_resnet18/lr_0.1...\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Build model RESNET18...\n",
      "Loading model config from ./model_configs/resnet18.json\n",
      "########## RESNET18 CONFIG ################\n",
      "num_classes:\t10\n",
      "############################################\n",
      "Initialized RESNET18 model with 11173962 total parameters, of which 11173962 are learnable.\n",
      "====== Epoch 0 ======>\n",
      "[TRAIN] Epoch: 0, Iter: 0, Loss: 2.35367\n",
      "[TRAIN] Epoch: 0, Iter: 80, Loss: nan\n",
      "[TRAIN] Epoch: 0, Iter: 160, Loss: nan\n",
      "[TRAIN] Epoch: 0, Iter: 240, Loss: nan\n",
      "[TRAIN] Epoch: 0, Iter: 320, Loss: nan\n",
      "== [TRAIN] Epoch: 0, Accuracy: 0.100 ==>\n",
      "[VAL] Epoch: 0, Iter: 0, Loss: nan\n",
      "=== [VAL] Epoch: 0, Iter: 39, Accuracy: 0.103 ===>\n",
      "====== Epoch 1 ======>\n",
      "[TRAIN] Epoch: 1, Iter: 0, Loss: nan\n",
      "[TRAIN] Epoch: 1, Iter: 80, Loss: nan\n",
      "[TRAIN] Epoch: 1, Iter: 160, Loss: nan\n",
      "[TRAIN] Epoch: 1, Iter: 240, Loss: nan\n",
      "[TRAIN] Epoch: 1, Iter: 320, Loss: nan\n",
      "== [TRAIN] Epoch: 1, Accuracy: 0.100 ==>\n",
      "[VAL] Epoch: 1, Iter: 0, Loss: nan\n",
      "=== [VAL] Epoch: 1, Iter: 39, Accuracy: 0.103 ===>\n",
      "====== Epoch 2 ======>\n",
      "[TRAIN] Epoch: 2, Iter: 0, Loss: nan\n",
      "[TRAIN] Epoch: 2, Iter: 80, Loss: nan\n",
      "[TRAIN] Epoch: 2, Iter: 160, Loss: nan\n",
      "[TRAIN] Epoch: 2, Iter: 240, Loss: nan\n",
      "[TRAIN] Epoch: 2, Iter: 320, Loss: nan\n",
      "== [TRAIN] Epoch: 2, Accuracy: 0.100 ==>\n",
      "[VAL] Epoch: 2, Iter: 0, Loss: nan\n",
      "=== [VAL] Epoch: 2, Iter: 39, Accuracy: 0.103 ===>\n",
      "====== Epoch 3 ======>\n",
      "[TRAIN] Epoch: 3, Iter: 0, Loss: nan\n",
      "[TRAIN] Epoch: 3, Iter: 80, Loss: nan\n",
      "[TRAIN] Epoch: 3, Iter: 160, Loss: nan\n",
      "[TRAIN] Epoch: 3, Iter: 240, Loss: nan\n",
      "[TRAIN] Epoch: 3, Iter: 320, Loss: nan\n",
      "== [TRAIN] Epoch: 3, Accuracy: 0.100 ==>\n",
      "[VAL] Epoch: 3, Iter: 0, Loss: nan\n",
      "=== [VAL] Epoch: 3, Iter: 39, Accuracy: 0.103 ===>\n",
      "====== Epoch 4 ======>\n",
      "[TRAIN] Epoch: 4, Iter: 0, Loss: nan\n",
      "[TRAIN] Epoch: 4, Iter: 80, Loss: nan\n",
      "[TRAIN] Epoch: 4, Iter: 160, Loss: nan\n",
      "[TRAIN] Epoch: 4, Iter: 240, Loss: nan\n",
      "[TRAIN] Epoch: 4, Iter: 320, Loss: nan\n",
      "== [TRAIN] Epoch: 4, Accuracy: 0.100 ==>\n",
      "[VAL] Epoch: 4, Iter: 0, Loss: nan\n",
      "=== [VAL] Epoch: 4, Iter: 39, Accuracy: 0.103 ===>\n",
      "====== Epoch 5 ======>\n",
      "[TRAIN] Epoch: 5, Iter: 0, Loss: nan\n",
      "[TRAIN] Epoch: 5, Iter: 80, Loss: nan\n",
      "[TRAIN] Epoch: 5, Iter: 160, Loss: nan\n",
      "[TRAIN] Epoch: 5, Iter: 240, Loss: nan\n",
      "[TRAIN] Epoch: 5, Iter: 320, Loss: nan\n",
      "== [TRAIN] Epoch: 5, Accuracy: 0.100 ==>\n",
      "[VAL] Epoch: 5, Iter: 0, Loss: nan\n",
      "=== [VAL] Epoch: 5, Iter: 39, Accuracy: 0.103 ===>\n",
      "====== Epoch 6 ======>\n",
      "[TRAIN] Epoch: 6, Iter: 0, Loss: nan\n",
      "[TRAIN] Epoch: 6, Iter: 80, Loss: nan\n",
      "[TRAIN] Epoch: 6, Iter: 160, Loss: nan\n",
      "[TRAIN] Epoch: 6, Iter: 240, Loss: nan\n",
      "[TRAIN] Epoch: 6, Iter: 320, Loss: nan\n",
      "== [TRAIN] Epoch: 6, Accuracy: 0.100 ==>\n",
      "[VAL] Epoch: 6, Iter: 0, Loss: nan\n",
      "=== [VAL] Epoch: 6, Iter: 39, Accuracy: 0.103 ===>\n",
      "====== Epoch 7 ======>\n",
      "[TRAIN] Epoch: 7, Iter: 0, Loss: nan\n",
      "[TRAIN] Epoch: 7, Iter: 80, Loss: nan\n",
      "[TRAIN] Epoch: 7, Iter: 160, Loss: nan\n",
      "[TRAIN] Epoch: 7, Iter: 240, Loss: nan\n",
      "[TRAIN] Epoch: 7, Iter: 320, Loss: nan\n",
      "== [TRAIN] Epoch: 7, Accuracy: 0.100 ==>\n",
      "[VAL] Epoch: 7, Iter: 0, Loss: nan\n",
      "=== [VAL] Epoch: 7, Iter: 39, Accuracy: 0.103 ===>\n",
      "====== Epoch 8 ======>\n",
      "[TRAIN] Epoch: 8, Iter: 0, Loss: nan\n",
      "[TRAIN] Epoch: 8, Iter: 80, Loss: nan\n",
      "[TRAIN] Epoch: 8, Iter: 160, Loss: nan\n",
      "[TRAIN] Epoch: 8, Iter: 240, Loss: nan\n",
      "[TRAIN] Epoch: 8, Iter: 320, Loss: nan\n",
      "== [TRAIN] Epoch: 8, Accuracy: 0.100 ==>\n",
      "[VAL] Epoch: 8, Iter: 0, Loss: nan\n",
      "=== [VAL] Epoch: 8, Iter: 39, Accuracy: 0.103 ===>\n",
      "====== Epoch 9 ======>\n",
      "[TRAIN] Epoch: 9, Iter: 0, Loss: nan\n",
      "[TRAIN] Epoch: 9, Iter: 80, Loss: nan\n",
      "[TRAIN] Epoch: 9, Iter: 160, Loss: nan\n",
      "[TRAIN] Epoch: 9, Iter: 240, Loss: nan\n",
      "[TRAIN] Epoch: 9, Iter: 320, Loss: nan\n",
      "== [TRAIN] Epoch: 9, Accuracy: 0.100 ==>\n",
      "[VAL] Epoch: 9, Iter: 0, Loss: nan\n",
      "=== [VAL] Epoch: 9, Iter: 39, Accuracy: 0.103 ===>\n",
      "====== Epoch 10 ======>\n",
      "[TRAIN] Epoch: 10, Iter: 0, Loss: nan\n",
      "[TRAIN] Epoch: 10, Iter: 80, Loss: nan\n",
      "[TRAIN] Epoch: 10, Iter: 160, Loss: nan\n",
      "[TRAIN] Epoch: 10, Iter: 240, Loss: nan\n",
      "[TRAIN] Epoch: 10, Iter: 320, Loss: nan\n",
      "== [TRAIN] Epoch: 10, Accuracy: 0.100 ==>\n",
      "[VAL] Epoch: 10, Iter: 0, Loss: nan\n",
      "=== [VAL] Epoch: 10, Iter: 39, Accuracy: 0.103 ===>\n",
      "====== Epoch 11 ======>\n",
      "[TRAIN] Epoch: 11, Iter: 0, Loss: nan\n",
      "[TRAIN] Epoch: 11, Iter: 80, Loss: nan\n",
      "[TRAIN] Epoch: 11, Iter: 160, Loss: nan\n",
      "[TRAIN] Epoch: 11, Iter: 240, Loss: nan\n",
      "[TRAIN] Epoch: 11, Iter: 320, Loss: nan\n",
      "== [TRAIN] Epoch: 11, Accuracy: 0.100 ==>\n",
      "[VAL] Epoch: 11, Iter: 0, Loss: nan\n",
      "=== [VAL] Epoch: 11, Iter: 39, Accuracy: 0.103 ===>\n",
      "====== Epoch 12 ======>\n",
      "[TRAIN] Epoch: 12, Iter: 0, Loss: nan\n",
      "[TRAIN] Epoch: 12, Iter: 80, Loss: nan\n",
      "[TRAIN] Epoch: 12, Iter: 160, Loss: nan\n",
      "[TRAIN] Epoch: 12, Iter: 240, Loss: nan\n",
      "[TRAIN] Epoch: 12, Iter: 320, Loss: nan\n",
      "== [TRAIN] Epoch: 12, Accuracy: 0.100 ==>\n",
      "[VAL] Epoch: 12, Iter: 0, Loss: nan\n",
      "=== [VAL] Epoch: 12, Iter: 39, Accuracy: 0.103 ===>\n",
      "====== Epoch 13 ======>\n",
      "[TRAIN] Epoch: 13, Iter: 0, Loss: nan\n",
      "[TRAIN] Epoch: 13, Iter: 80, Loss: nan\n",
      "[TRAIN] Epoch: 13, Iter: 160, Loss: nan\n",
      "[TRAIN] Epoch: 13, Iter: 240, Loss: nan\n",
      "[TRAIN] Epoch: 13, Iter: 320, Loss: nan\n",
      "== [TRAIN] Epoch: 13, Accuracy: 0.100 ==>\n",
      "[VAL] Epoch: 13, Iter: 0, Loss: nan\n",
      "=== [VAL] Epoch: 13, Iter: 39, Accuracy: 0.103 ===>\n",
      "====== Epoch 14 ======>\n",
      "[TRAIN] Epoch: 14, Iter: 0, Loss: nan\n",
      "[TRAIN] Epoch: 14, Iter: 80, Loss: nan\n",
      "[TRAIN] Epoch: 14, Iter: 160, Loss: nan\n",
      "[TRAIN] Epoch: 14, Iter: 240, Loss: nan\n",
      "[TRAIN] Epoch: 14, Iter: 320, Loss: nan\n",
      "== [TRAIN] Epoch: 14, Accuracy: 0.100 ==>\n",
      "[VAL] Epoch: 14, Iter: 0, Loss: nan\n",
      "=== [VAL] Epoch: 14, Iter: 39, Accuracy: 0.103 ===>\n",
      "[TEST] Epoch: 14, Iter: 0, Loss: nan\n",
      "=== [TEST] Epoch: 14, Iter: 78, Accuracy: 0.100 ===>\n",
      "===== Best validation Accuracy: 0.103 =====>\n",
      "Writing training logs to logs_resnet18/lr_0.01...\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Build model RESNET18...\n",
      "Loading model config from ./model_configs/resnet18.json\n",
      "########## RESNET18 CONFIG ################\n",
      "num_classes:\t10\n",
      "############################################\n",
      "Initialized RESNET18 model with 11173962 total parameters, of which 11173962 are learnable.\n",
      "====== Epoch 0 ======>\n",
      "[TRAIN] Epoch: 0, Iter: 0, Loss: 2.35367\n",
      "[TRAIN] Epoch: 0, Iter: 80, Loss: 1.47302\n",
      "[TRAIN] Epoch: 0, Iter: 160, Loss: 1.50006\n",
      "[TRAIN] Epoch: 0, Iter: 240, Loss: 1.34884\n",
      "[TRAIN] Epoch: 0, Iter: 320, Loss: 1.24164\n",
      "== [TRAIN] Epoch: 0, Accuracy: 0.466 ==>\n",
      "[VAL] Epoch: 0, Iter: 0, Loss: 1.24098\n",
      "=== [VAL] Epoch: 0, Iter: 39, Accuracy: 0.583 ===>\n",
      "====== Epoch 1 ======>\n",
      "[TRAIN] Epoch: 1, Iter: 0, Loss: 1.06029\n",
      "[TRAIN] Epoch: 1, Iter: 80, Loss: 0.99294\n",
      "[TRAIN] Epoch: 1, Iter: 160, Loss: 0.95667\n",
      "[TRAIN] Epoch: 1, Iter: 240, Loss: 0.99754\n",
      "[TRAIN] Epoch: 1, Iter: 320, Loss: 0.78610\n",
      "== [TRAIN] Epoch: 1, Accuracy: 0.657 ==>\n",
      "[VAL] Epoch: 1, Iter: 0, Loss: 0.90569\n",
      "=== [VAL] Epoch: 1, Iter: 39, Accuracy: 0.664 ===>\n",
      "====== Epoch 2 ======>\n",
      "[TRAIN] Epoch: 2, Iter: 0, Loss: 0.97644\n",
      "[TRAIN] Epoch: 2, Iter: 80, Loss: 0.77329\n",
      "[TRAIN] Epoch: 2, Iter: 160, Loss: 0.80272\n",
      "[TRAIN] Epoch: 2, Iter: 240, Loss: 0.75576\n",
      "[TRAIN] Epoch: 2, Iter: 320, Loss: 0.71588\n",
      "== [TRAIN] Epoch: 2, Accuracy: 0.739 ==>\n",
      "[VAL] Epoch: 2, Iter: 0, Loss: 0.78493\n",
      "=== [VAL] Epoch: 2, Iter: 39, Accuracy: 0.726 ===>\n",
      "====== Epoch 3 ======>\n",
      "[TRAIN] Epoch: 3, Iter: 0, Loss: 0.63341\n",
      "[TRAIN] Epoch: 3, Iter: 80, Loss: 0.67383\n",
      "[TRAIN] Epoch: 3, Iter: 160, Loss: 0.65223\n",
      "[TRAIN] Epoch: 3, Iter: 240, Loss: 0.46595\n",
      "[TRAIN] Epoch: 3, Iter: 320, Loss: 0.65129\n",
      "== [TRAIN] Epoch: 3, Accuracy: 0.784 ==>\n",
      "[VAL] Epoch: 3, Iter: 0, Loss: 0.75726\n",
      "=== [VAL] Epoch: 3, Iter: 39, Accuracy: 0.789 ===>\n",
      "====== Epoch 4 ======>\n",
      "[TRAIN] Epoch: 4, Iter: 0, Loss: 0.53324\n",
      "[TRAIN] Epoch: 4, Iter: 80, Loss: 0.54068\n",
      "[TRAIN] Epoch: 4, Iter: 160, Loss: 0.60213\n",
      "[TRAIN] Epoch: 4, Iter: 240, Loss: 0.53519\n",
      "[TRAIN] Epoch: 4, Iter: 320, Loss: 0.47082\n",
      "== [TRAIN] Epoch: 4, Accuracy: 0.813 ==>\n",
      "[VAL] Epoch: 4, Iter: 0, Loss: 0.59230\n",
      "=== [VAL] Epoch: 4, Iter: 39, Accuracy: 0.808 ===>\n",
      "====== Epoch 5 ======>\n",
      "[TRAIN] Epoch: 5, Iter: 0, Loss: 0.38582\n",
      "[TRAIN] Epoch: 5, Iter: 80, Loss: 0.39626\n",
      "[TRAIN] Epoch: 5, Iter: 160, Loss: 0.52894\n",
      "[TRAIN] Epoch: 5, Iter: 240, Loss: 0.50418\n",
      "[TRAIN] Epoch: 5, Iter: 320, Loss: 0.26605\n",
      "== [TRAIN] Epoch: 5, Accuracy: 0.835 ==>\n",
      "[VAL] Epoch: 5, Iter: 0, Loss: 0.39468\n",
      "=== [VAL] Epoch: 5, Iter: 39, Accuracy: 0.846 ===>\n",
      "====== Epoch 6 ======>\n",
      "[TRAIN] Epoch: 6, Iter: 0, Loss: 0.54015\n",
      "[TRAIN] Epoch: 6, Iter: 80, Loss: 0.46644\n",
      "[TRAIN] Epoch: 6, Iter: 160, Loss: 0.29336\n",
      "[TRAIN] Epoch: 6, Iter: 240, Loss: 0.40851\n",
      "[TRAIN] Epoch: 6, Iter: 320, Loss: 0.26758\n",
      "== [TRAIN] Epoch: 6, Accuracy: 0.855 ==>\n",
      "[VAL] Epoch: 6, Iter: 0, Loss: 0.50406\n",
      "=== [VAL] Epoch: 6, Iter: 39, Accuracy: 0.852 ===>\n",
      "====== Epoch 7 ======>\n",
      "[TRAIN] Epoch: 7, Iter: 0, Loss: 0.35159\n",
      "[TRAIN] Epoch: 7, Iter: 80, Loss: 0.36025\n",
      "[TRAIN] Epoch: 7, Iter: 160, Loss: 0.31335\n",
      "[TRAIN] Epoch: 7, Iter: 240, Loss: 0.40162\n",
      "[TRAIN] Epoch: 7, Iter: 320, Loss: 0.39576\n",
      "== [TRAIN] Epoch: 7, Accuracy: 0.866 ==>\n",
      "[VAL] Epoch: 7, Iter: 0, Loss: 0.36282\n",
      "=== [VAL] Epoch: 7, Iter: 39, Accuracy: 0.885 ===>\n",
      "====== Epoch 8 ======>\n",
      "[TRAIN] Epoch: 8, Iter: 0, Loss: 0.47826\n",
      "[TRAIN] Epoch: 8, Iter: 80, Loss: 0.29508\n",
      "[TRAIN] Epoch: 8, Iter: 160, Loss: 0.25591\n",
      "[TRAIN] Epoch: 8, Iter: 240, Loss: 0.45391\n",
      "[TRAIN] Epoch: 8, Iter: 320, Loss: 0.43866\n",
      "== [TRAIN] Epoch: 8, Accuracy: 0.875 ==>\n",
      "[VAL] Epoch: 8, Iter: 0, Loss: 0.37741\n",
      "=== [VAL] Epoch: 8, Iter: 39, Accuracy: 0.864 ===>\n",
      "====== Epoch 9 ======>\n",
      "[TRAIN] Epoch: 9, Iter: 0, Loss: 0.35610\n",
      "[TRAIN] Epoch: 9, Iter: 80, Loss: 0.38551\n",
      "[TRAIN] Epoch: 9, Iter: 160, Loss: 0.44980\n",
      "[TRAIN] Epoch: 9, Iter: 240, Loss: 0.43843\n",
      "[TRAIN] Epoch: 9, Iter: 320, Loss: 0.39833\n",
      "== [TRAIN] Epoch: 9, Accuracy: 0.888 ==>\n",
      "[VAL] Epoch: 9, Iter: 0, Loss: 0.18398\n",
      "=== [VAL] Epoch: 9, Iter: 39, Accuracy: 0.899 ===>\n",
      "====== Epoch 10 ======>\n",
      "[TRAIN] Epoch: 10, Iter: 0, Loss: 0.20392\n",
      "[TRAIN] Epoch: 10, Iter: 80, Loss: 0.33596\n",
      "[TRAIN] Epoch: 10, Iter: 160, Loss: 0.33420\n",
      "[TRAIN] Epoch: 10, Iter: 240, Loss: 0.37917\n",
      "[TRAIN] Epoch: 10, Iter: 320, Loss: 0.20284\n",
      "== [TRAIN] Epoch: 10, Accuracy: 0.896 ==>\n",
      "[VAL] Epoch: 10, Iter: 0, Loss: 0.30015\n",
      "=== [VAL] Epoch: 10, Iter: 39, Accuracy: 0.891 ===>\n",
      "====== Epoch 11 ======>\n",
      "[TRAIN] Epoch: 11, Iter: 0, Loss: 0.16509\n",
      "[TRAIN] Epoch: 11, Iter: 80, Loss: 0.24073\n",
      "[TRAIN] Epoch: 11, Iter: 160, Loss: 0.32276\n",
      "[TRAIN] Epoch: 11, Iter: 240, Loss: 0.24990\n",
      "[TRAIN] Epoch: 11, Iter: 320, Loss: 0.40184\n",
      "== [TRAIN] Epoch: 11, Accuracy: 0.904 ==>\n",
      "[VAL] Epoch: 11, Iter: 0, Loss: 0.25474\n",
      "=== [VAL] Epoch: 11, Iter: 39, Accuracy: 0.893 ===>\n",
      "====== Epoch 12 ======>\n",
      "[TRAIN] Epoch: 12, Iter: 0, Loss: 0.23252\n",
      "[TRAIN] Epoch: 12, Iter: 80, Loss: 0.14732\n",
      "[TRAIN] Epoch: 12, Iter: 160, Loss: 0.29468\n",
      "[TRAIN] Epoch: 12, Iter: 240, Loss: 0.22295\n",
      "[TRAIN] Epoch: 12, Iter: 320, Loss: 0.32074\n",
      "== [TRAIN] Epoch: 12, Accuracy: 0.912 ==>\n",
      "[VAL] Epoch: 12, Iter: 0, Loss: 0.23276\n",
      "=== [VAL] Epoch: 12, Iter: 39, Accuracy: 0.931 ===>\n",
      "====== Epoch 13 ======>\n",
      "[TRAIN] Epoch: 13, Iter: 0, Loss: 0.21839\n",
      "[TRAIN] Epoch: 13, Iter: 80, Loss: 0.27954\n",
      "[TRAIN] Epoch: 13, Iter: 160, Loss: 0.26712\n",
      "[TRAIN] Epoch: 13, Iter: 240, Loss: 0.22267\n",
      "[TRAIN] Epoch: 13, Iter: 320, Loss: 0.21136\n",
      "== [TRAIN] Epoch: 13, Accuracy: 0.921 ==>\n",
      "[VAL] Epoch: 13, Iter: 0, Loss: 0.21853\n",
      "=== [VAL] Epoch: 13, Iter: 39, Accuracy: 0.927 ===>\n",
      "====== Epoch 14 ======>\n",
      "[TRAIN] Epoch: 14, Iter: 0, Loss: 0.25128\n",
      "[TRAIN] Epoch: 14, Iter: 80, Loss: 0.29631\n",
      "[TRAIN] Epoch: 14, Iter: 160, Loss: 0.31339\n",
      "[TRAIN] Epoch: 14, Iter: 240, Loss: 0.13387\n",
      "[TRAIN] Epoch: 14, Iter: 320, Loss: 0.24153\n",
      "== [TRAIN] Epoch: 14, Accuracy: 0.924 ==>\n",
      "[VAL] Epoch: 14, Iter: 0, Loss: 0.21462\n",
      "=== [VAL] Epoch: 14, Iter: 39, Accuracy: 0.941 ===>\n",
      "[TEST] Epoch: 14, Iter: 0, Loss: 0.26240\n",
      "=== [TEST] Epoch: 14, Iter: 78, Accuracy: 0.878 ===>\n",
      "===== Best validation Accuracy: 0.941 =====>\n",
      "Writing training logs to logs_resnet18/lr_0.001...\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Build model RESNET18...\n",
      "Loading model config from ./model_configs/resnet18.json\n",
      "########## RESNET18 CONFIG ################\n",
      "num_classes:\t10\n",
      "############################################\n",
      "Initialized RESNET18 model with 11173962 total parameters, of which 11173962 are learnable.\n",
      "====== Epoch 0 ======>\n",
      "[TRAIN] Epoch: 0, Iter: 0, Loss: 2.35367\n",
      "[TRAIN] Epoch: 0, Iter: 80, Loss: 1.42160\n",
      "[TRAIN] Epoch: 0, Iter: 160, Loss: 1.50792\n",
      "[TRAIN] Epoch: 0, Iter: 240, Loss: 1.19513\n",
      "[TRAIN] Epoch: 0, Iter: 320, Loss: 1.07011\n",
      "== [TRAIN] Epoch: 0, Accuracy: 0.510 ==>\n",
      "[VAL] Epoch: 0, Iter: 0, Loss: 1.24626\n",
      "=== [VAL] Epoch: 0, Iter: 39, Accuracy: 0.605 ===>\n",
      "====== Epoch 1 ======>\n",
      "[TRAIN] Epoch: 1, Iter: 0, Loss: 0.94366\n",
      "[TRAIN] Epoch: 1, Iter: 80, Loss: 0.91721\n",
      "[TRAIN] Epoch: 1, Iter: 160, Loss: 0.93366\n",
      "[TRAIN] Epoch: 1, Iter: 240, Loss: 0.85553\n",
      "[TRAIN] Epoch: 1, Iter: 320, Loss: 0.75335\n",
      "== [TRAIN] Epoch: 1, Accuracy: 0.666 ==>\n",
      "[VAL] Epoch: 1, Iter: 0, Loss: 1.06990\n",
      "=== [VAL] Epoch: 1, Iter: 39, Accuracy: 0.673 ===>\n",
      "====== Epoch 2 ======>\n",
      "[TRAIN] Epoch: 2, Iter: 0, Loss: 0.96542\n",
      "[TRAIN] Epoch: 2, Iter: 80, Loss: 0.78851\n",
      "[TRAIN] Epoch: 2, Iter: 160, Loss: 0.85565\n",
      "[TRAIN] Epoch: 2, Iter: 240, Loss: 0.70774\n",
      "[TRAIN] Epoch: 2, Iter: 320, Loss: 0.67740\n",
      "== [TRAIN] Epoch: 2, Accuracy: 0.738 ==>\n",
      "[VAL] Epoch: 2, Iter: 0, Loss: 0.86323\n",
      "=== [VAL] Epoch: 2, Iter: 39, Accuracy: 0.719 ===>\n",
      "====== Epoch 3 ======>\n",
      "[TRAIN] Epoch: 3, Iter: 0, Loss: 0.69002\n",
      "[TRAIN] Epoch: 3, Iter: 80, Loss: 0.52635\n",
      "[TRAIN] Epoch: 3, Iter: 160, Loss: 0.71462\n",
      "[TRAIN] Epoch: 3, Iter: 240, Loss: 0.62206\n",
      "[TRAIN] Epoch: 3, Iter: 320, Loss: 0.67640\n",
      "== [TRAIN] Epoch: 3, Accuracy: 0.782 ==>\n",
      "[VAL] Epoch: 3, Iter: 0, Loss: 0.61430\n",
      "=== [VAL] Epoch: 3, Iter: 39, Accuracy: 0.792 ===>\n",
      "====== Epoch 4 ======>\n",
      "[TRAIN] Epoch: 4, Iter: 0, Loss: 0.51326\n",
      "[TRAIN] Epoch: 4, Iter: 80, Loss: 0.61612\n",
      "[TRAIN] Epoch: 4, Iter: 160, Loss: 0.56225\n",
      "[TRAIN] Epoch: 4, Iter: 240, Loss: 0.43531\n",
      "[TRAIN] Epoch: 4, Iter: 320, Loss: 0.51312\n",
      "== [TRAIN] Epoch: 4, Accuracy: 0.809 ==>\n",
      "[VAL] Epoch: 4, Iter: 0, Loss: 0.49172\n",
      "=== [VAL] Epoch: 4, Iter: 39, Accuracy: 0.809 ===>\n",
      "====== Epoch 5 ======>\n",
      "[TRAIN] Epoch: 5, Iter: 0, Loss: 0.48976\n",
      "[TRAIN] Epoch: 5, Iter: 80, Loss: 0.33335\n",
      "[TRAIN] Epoch: 5, Iter: 160, Loss: 0.53490\n",
      "[TRAIN] Epoch: 5, Iter: 240, Loss: 0.50147\n",
      "[TRAIN] Epoch: 5, Iter: 320, Loss: 0.32495\n",
      "== [TRAIN] Epoch: 5, Accuracy: 0.829 ==>\n",
      "[VAL] Epoch: 5, Iter: 0, Loss: 0.41623\n",
      "=== [VAL] Epoch: 5, Iter: 39, Accuracy: 0.831 ===>\n",
      "====== Epoch 6 ======>\n",
      "[TRAIN] Epoch: 6, Iter: 0, Loss: 0.50007\n",
      "[TRAIN] Epoch: 6, Iter: 80, Loss: 0.42174\n",
      "[TRAIN] Epoch: 6, Iter: 160, Loss: 0.36903\n",
      "[TRAIN] Epoch: 6, Iter: 240, Loss: 0.44972\n",
      "[TRAIN] Epoch: 6, Iter: 320, Loss: 0.27522\n",
      "== [TRAIN] Epoch: 6, Accuracy: 0.848 ==>\n",
      "[VAL] Epoch: 6, Iter: 0, Loss: 0.45457\n",
      "=== [VAL] Epoch: 6, Iter: 39, Accuracy: 0.841 ===>\n",
      "====== Epoch 7 ======>\n",
      "[TRAIN] Epoch: 7, Iter: 0, Loss: 0.41205\n",
      "[TRAIN] Epoch: 7, Iter: 80, Loss: 0.38441\n",
      "[TRAIN] Epoch: 7, Iter: 160, Loss: 0.34951\n",
      "[TRAIN] Epoch: 7, Iter: 240, Loss: 0.36508\n",
      "[TRAIN] Epoch: 7, Iter: 320, Loss: 0.40386\n",
      "== [TRAIN] Epoch: 7, Accuracy: 0.863 ==>\n",
      "[VAL] Epoch: 7, Iter: 0, Loss: 0.43826\n",
      "=== [VAL] Epoch: 7, Iter: 39, Accuracy: 0.853 ===>\n",
      "====== Epoch 8 ======>\n",
      "[TRAIN] Epoch: 8, Iter: 0, Loss: 0.40968\n",
      "[TRAIN] Epoch: 8, Iter: 80, Loss: 0.32046\n",
      "[TRAIN] Epoch: 8, Iter: 160, Loss: 0.30436\n",
      "[TRAIN] Epoch: 8, Iter: 240, Loss: 0.39277\n",
      "[TRAIN] Epoch: 8, Iter: 320, Loss: 0.42780\n",
      "== [TRAIN] Epoch: 8, Accuracy: 0.875 ==>\n",
      "[VAL] Epoch: 8, Iter: 0, Loss: 0.38790\n",
      "=== [VAL] Epoch: 8, Iter: 39, Accuracy: 0.869 ===>\n",
      "====== Epoch 9 ======>\n",
      "[TRAIN] Epoch: 9, Iter: 0, Loss: 0.22938\n",
      "[TRAIN] Epoch: 9, Iter: 80, Loss: 0.35159\n",
      "[TRAIN] Epoch: 9, Iter: 160, Loss: 0.45255\n",
      "[TRAIN] Epoch: 9, Iter: 240, Loss: 0.44704\n",
      "[TRAIN] Epoch: 9, Iter: 320, Loss: 0.37706\n",
      "== [TRAIN] Epoch: 9, Accuracy: 0.887 ==>\n",
      "[VAL] Epoch: 9, Iter: 0, Loss: 0.22867\n",
      "=== [VAL] Epoch: 9, Iter: 39, Accuracy: 0.902 ===>\n",
      "====== Epoch 10 ======>\n",
      "[TRAIN] Epoch: 10, Iter: 0, Loss: 0.20222\n",
      "[TRAIN] Epoch: 10, Iter: 80, Loss: 0.32137\n",
      "[TRAIN] Epoch: 10, Iter: 160, Loss: 0.31380\n",
      "[TRAIN] Epoch: 10, Iter: 240, Loss: 0.31413\n",
      "[TRAIN] Epoch: 10, Iter: 320, Loss: 0.27131\n",
      "== [TRAIN] Epoch: 10, Accuracy: 0.897 ==>\n",
      "[VAL] Epoch: 10, Iter: 0, Loss: 0.29466\n",
      "=== [VAL] Epoch: 10, Iter: 39, Accuracy: 0.907 ===>\n",
      "====== Epoch 11 ======>\n",
      "[TRAIN] Epoch: 11, Iter: 0, Loss: 0.17318\n",
      "[TRAIN] Epoch: 11, Iter: 80, Loss: 0.27316\n",
      "[TRAIN] Epoch: 11, Iter: 160, Loss: 0.32609\n",
      "[TRAIN] Epoch: 11, Iter: 240, Loss: 0.24924\n",
      "[TRAIN] Epoch: 11, Iter: 320, Loss: 0.32330\n",
      "== [TRAIN] Epoch: 11, Accuracy: 0.906 ==>\n",
      "[VAL] Epoch: 11, Iter: 0, Loss: 0.24136\n",
      "=== [VAL] Epoch: 11, Iter: 39, Accuracy: 0.900 ===>\n",
      "====== Epoch 12 ======>\n",
      "[TRAIN] Epoch: 12, Iter: 0, Loss: 0.20864\n",
      "[TRAIN] Epoch: 12, Iter: 80, Loss: 0.20567\n",
      "[TRAIN] Epoch: 12, Iter: 160, Loss: 0.19857\n",
      "[TRAIN] Epoch: 12, Iter: 240, Loss: 0.25403\n",
      "[TRAIN] Epoch: 12, Iter: 320, Loss: 0.27654\n",
      "== [TRAIN] Epoch: 12, Accuracy: 0.914 ==>\n",
      "[VAL] Epoch: 12, Iter: 0, Loss: 0.28144\n",
      "=== [VAL] Epoch: 12, Iter: 39, Accuracy: 0.930 ===>\n",
      "====== Epoch 13 ======>\n",
      "[TRAIN] Epoch: 13, Iter: 0, Loss: 0.26835\n",
      "[TRAIN] Epoch: 13, Iter: 80, Loss: 0.21924\n",
      "[TRAIN] Epoch: 13, Iter: 160, Loss: 0.28513\n",
      "[TRAIN] Epoch: 13, Iter: 240, Loss: 0.25848\n",
      "[TRAIN] Epoch: 13, Iter: 320, Loss: 0.25674\n",
      "== [TRAIN] Epoch: 13, Accuracy: 0.923 ==>\n",
      "[VAL] Epoch: 13, Iter: 0, Loss: 0.22304\n",
      "=== [VAL] Epoch: 13, Iter: 39, Accuracy: 0.929 ===>\n",
      "====== Epoch 14 ======>\n",
      "[TRAIN] Epoch: 14, Iter: 0, Loss: 0.15257\n"
     ]
    }
   ],
   "source": [
    "# Ex√©cuter l'exp√©rience pour chaque taux d'apprentissage\n",
    "for lr in learning_rates:\n",
    "    train_with_lr(lr, logdir)\n",
    "    \n",
    "# G√©n√©ration des graphiques\n",
    "list_of_dirs = [os.path.join(logdir, f\"lr_{lr}\") for lr in learning_rates]\n",
    "legend_names = [f\"lr={lr}\" for lr in learning_rates]\n",
    "generate_plots(list_of_dirs, legend_names, logdir)\n",
    "\n",
    "print(\"Exp√©rience termin√©e. Les graphiques sont enregistr√©s dans\", logdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Entra√Ænement avec patch_size=4...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16526/812909305.py:6: UserWarning: CUDA is not available, make that your environment is running on GPU (e.g. in the Notebook Settings in Google Colab). Forcing device=\"cpu\".\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_16526/812909305.py:14: UserWarning: You are about to run on CPU, and might run out of memory shortly. You can try setting batch_size=1 to reduce memory usage.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Build model MLPMIXER...\n",
      "Loading model config from logs_mlpmixer/patch_4/temp_mlpmixer.json\n",
      "########## MLPMIXER CONFIG ################\n",
      "num_classes:\t10\n",
      "img_size:\t32\n",
      "patch_size:\t4\n",
      "embed_dim:\t256\n",
      "num_blocks:\t4\n",
      "drop_rate:\t0.0\n",
      "activation:\tgelu\n",
      "############################################\n",
      "Initialized MLPMIXER model with 2188298 total parameters, of which 2188298 are learnable.\n",
      "====== Epoch 0 ======>\n",
      "[TRAIN] Epoch: 0, Iter: 0, Loss: 2.31901\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m     args\u001b[38;5;241m.\u001b[39mmodel_config \u001b[38;5;241m=\u001b[39m temp_config_path  \u001b[38;5;66;03m# Utiliser le fichier temporaire\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# Lancer l'entra√Ænement avec `main_entry(args)`\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m     \u001b[43mmain_entry\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# G√©n√©rer les graphiques apr√®s toutes les exp√©riences\u001b[39;00m\n\u001b[1;32m     29\u001b[0m list_of_dirs \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(logdir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpatch_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpatch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m patch \u001b[38;5;129;01min\u001b[39;00m patch_sizes]\n",
      "Cell \u001b[0;32mIn[6], line 91\u001b[0m, in \u001b[0;36mmain_entry\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(args\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[1;32m     90\u001b[0m     tqdm\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m====== Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ======>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 91\u001b[0m     loss, acc, wall_time \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[1;32m     93\u001b[0m     train_accs\u001b[38;5;241m.\u001b[39mappend(acc)\n",
      "File \u001b[0;32m~/Classification-d-image-avec-plusieurs-modele/HW1_2025/assignment1_release/main.py:37\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch, model, dataloader, optimizer, args)\u001b[0m\n\u001b[1;32m     34\u001b[0m loss \u001b[38;5;241m=\u001b[39m cross_entropy_loss(logits, labels)\n\u001b[1;32m     35\u001b[0m acc \u001b[38;5;241m=\u001b[39m compute_accuracy(logits, labels)\n\u001b[0;32m---> 37\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     39\u001b[0m epoch_accuracy \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m acc\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# D√©finition des tailles de patch √† tester\n",
    "patch_sizes = [4, 8, 16]  # Tailles valides pour CIFAR-10 (32x32)\n",
    "logdir = \"logs_mlpmixer\"\n",
    "os.makedirs(logdir, exist_ok=True)\n",
    "\n",
    "for patch in patch_sizes:\n",
    "    print(f\"\\nüîÑ Entra√Ænement avec patch_size={patch}...\\n\")\n",
    "    \n",
    "    args = Arguments()\n",
    "    args.model = 'mlpmixer'\n",
    "    args.optimizer = 'adam'\n",
    "    args.model_config = \"./model_configs/mlpmixer.json\"\n",
    "    args.logdir = os.path.join(logdir, f\"patch_{patch}\")\n",
    "    os.makedirs(args.logdir, exist_ok=True)\n",
    "    \n",
    "    # Modifier la configuration du mod√®le pour inclure la taille du patch\n",
    "    with open(args.model_config, 'r') as f:\n",
    "        model_config = json.load(f)\n",
    "    model_config[\"patch_size\"] = patch  # Mise √† jour de la taille du patch\n",
    "    temp_config_path = os.path.join(args.logdir, \"temp_mlpmixer.json\")\n",
    "    with open(temp_config_path, 'w') as f:\n",
    "        json.dump(model_config, f, indent=4)\n",
    "    args.model_config = temp_config_path  # Utiliser le fichier temporaire\n",
    "\n",
    "    # Lancer l'entra√Ænement avec `main_entry(args)`\n",
    "    main_entry(args)\n",
    "\n",
    "# G√©n√©rer les graphiques apr√®s toutes les exp√©riences\n",
    "list_of_dirs = [os.path.join(logdir, f\"patch_{patch}\") for patch in patch_sizes]\n",
    "legend_names = [f\"patch={patch}\" for patch in patch_sizes]\n",
    "generate_plots(list_of_dirs, legend_names, logdir)\n",
    "\n",
    "print(\"\\n‚úÖ Exp√©rience termin√©e. Les graphiques sont enregistr√©s dans\", logdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2 (default, Feb 26 2020, 14:31:49) \n[GCC 6.3.0 20170516]"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "81c564fb939afe7b3f114d194e01dc23538f9aaa81b9a9b61cd5d8751a87bdce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
