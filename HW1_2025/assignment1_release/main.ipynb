{"cells":[{"cell_type":"markdown","metadata":{"id":"uTv0D26B9W2h"},"source":["# Assignment 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qFHMMDtSwuW4"},"outputs":[],"source":["#@title Mount your Google Drive\n","# If you run this notebook locally or on a cluster (i.e. not on Google Colab)\n","# you can delete this cell which is specific to Google Colab. You may also\n","# change the paths for data/logs in Arguments below.\n","%matplotlib inline\n","%load_ext autoreload\n","%autoreload 2\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oODLwt1QzgGa"},"outputs":[],"source":["#@title Link your assignment folder & install requirements\n","#@markdown Enter the path to the assignment folder in your Google Drive\n","# If you run this notebook locally or on a cluster (i.e. not on Google Colab)\n","# you can delete this cell which is specific to Google Colab. You may also\n","# change the paths for data/logs in Arguments below.\n","import sys\n","import os\n","import shutil\n","import warnings\n","\n","folder = \"Your path to assignment folder\" #@param {type:\"string\"}\n","!ln -Ts \"$folder\" /content/assignment 2> /dev/null\n","\n","# Add the assignment folder to Python path\n","if '/content/assignment' not in sys.path:\n","  sys.path.insert(0, '/content/assignment')\n","\n","# Check if CUDA is available\n","import torch\n","if not torch.cuda.is_available():\n","  warnings.warn('CUDA is not available.')"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"dt3NTvpsy4Oc"},"source":["### Running on GPU\n","For this assignment, it will be necessary to run your experiments on GPU. To make sure the notebook is running on GPU, you can change the notebook settings with\n","* (EN) `Edit > Notebook Settings`\n","* (FR) `Modifier > ParamÃ¨tres du notebook`\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RLVSmv9HoMH5"},"outputs":[],"source":["%matplotlib inline\n","import warnings\n","\n","from dataclasses import dataclass\n","import torch\n","from torch import optim\n","from torchvision.datasets import CIFAR10\n","from torchvision import transforms\n","from utils import seed_experiment, to_device, cross_entropy_loss, compute_accuracy\n","from config import get_config_parser\n","import json\n","from mlp import MLP\n","from resnet18 import ResNet18\n","from mlpmixer import MLPMixer\n","from tqdm import tqdm\n","from torch.utils.data import DataLoader\n","import time\n","import os"]},{"cell_type":"markdown","metadata":{"id":"gZy1J-0OroLg"},"source":["# Local Test\n","Before run the experiment, here are some local test cases you can run for sanity check"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wLEVxwLlroLh"},"outputs":[],"source":["import unittest\n","import test\n","suite = unittest.TestLoader().loadTestsFromModule(test)\n","unittest.TextTestRunner(verbosity=2).run(suite)"]},{"cell_type":"markdown","metadata":{"id":"-PtvL_yKp3PW"},"source":["## Experiments"]},{"cell_type":"markdown","metadata":{"id":"iWiJme7XaLiR"},"source":["Below we define a few default arguments to get you started with your experiments. You are encouraged to modify the function `main_entry()`, as well as these arguments, to fit your needs (e.g. changing hyperparameters, the optimizer, adding regularizations)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YUrqebfCobD1"},"outputs":[],"source":["@dataclass\n","class Arguments:\n","  # Data\n","  batch_size: int = 128\n","  # Model\n","  model: str = 'mlp'  # [mlp, resnet18, mlpmixer]\n","  model_config: str = \"./model_configs/mlp.json\" # path to model config json file\n","\n","  # Optimization\n","  optimizer: str = 'adamw'  # [sgd, momentum, adam, adamw]\n","  epochs: int = 15\n","  lr: float = 1e-3\n","  momentum: float = 0.9\n","  weight_decay: float = 5e-4\n","\n","  # Experiment\n","  logdir: str = '/content/assignment/logs'\n","  seed: int = 42\n","\n","  # Miscellaneous\n","  device: str = 'cuda'\n","  visualize : bool = False\n","  print_every: int = 80"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g2rjoY-5phTY"},"outputs":[],"source":["# Main code entry. Train the model and save the logs\n","from main import train, evaluate\n","def main_entry(args):\n","    # Check for the device\n","    if (args.device == \"cuda\") and not torch.cuda.is_available():\n","        warnings.warn(\n","            \"CUDA is not available, make that your environment is \"\n","            \"running on GPU (e.g. in the Notebook Settings in Google Colab). \"\n","            'Forcing device=\"cpu\".'\n","        )\n","        args.device = \"cpu\"\n","\n","    if args.device == \"cpu\":\n","        warnings.warn(\n","            \"You are about to run on CPU, and might run out of memory \"\n","            \"shortly. You can try setting batch_size=1 to reduce memory usage.\"\n","        )\n","\n","    # Seed the experiment, for repeatability\n","    seed_experiment(args.seed)\n","\n","    test_transform = transforms.Compose([transforms.ToTensor(),\n","                                     transforms.Normalize([0.49139968, 0.48215841, 0.44653091], [0.24703223, 0.24348513, 0.26158784])\n","                                     ])\n","    # For training, we add some augmentation. Networks are too powerful and would overfit.\n","    train_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n","                                          transforms.RandomResizedCrop((32,32),scale=(0.8,1.0),ratio=(0.9,1.1)),\n","                                          transforms.ToTensor(),\n","                                          transforms.Normalize([0.49139968, 0.48215841, 0.44653091], [0.24703223, 0.24348513, 0.26158784])\n","                                        ])\n","    # Loading the training dataset. We need to split it into a training and validation part\n","    # We need to do a little trick because the validation set should not use the augmentation.\n","    train_dataset = CIFAR10(root='./data', train=True, transform=train_transform, download=True)\n","    val_dataset = CIFAR10(root='./data', train=True, transform=test_transform, download=True)\n","    train_set, _ = torch.utils.data.random_split(train_dataset, [45000, 5000])\n","    _, val_set = torch.utils.data.random_split(val_dataset, [45000, 5000])\n","\n","    # Loading the test set\n","    test_set = CIFAR10(root='./data', train=False, transform=test_transform, download=True)\n","    \n","    # Load model\n","    print(f'Build model {args.model.upper()}...')\n","    if args.model_config is not None:\n","        print(f'Loading model config from {args.model_config}')\n","        with open(args.model_config) as f:\n","            model_config = json.load(f)\n","    else:\n","        raise ValueError('Please provide a model config json')\n","    print(f'########## {args.model.upper()} CONFIG ################')\n","    for key, val in model_config.items():\n","        print(f'{key}:\\t{val}')\n","    print('############################################')\n","    model_cls = {'mlp': MLP, 'resnet18': ResNet18, 'mlpmixer': MLPMixer}[args.model]\n","    model = model_cls(**model_config)\n","    model.to(args.device)\n","    \n","    # Optimizer\n","    if args.optimizer == \"adamw\":\n","        optimizer = optim.AdamW(\n","            model.parameters(), lr=args.lr, weight_decay=args.weight_decay\n","        )\n","    elif args.optimizer == \"adam\":\n","        optimizer = optim.Adam(model.parameters(), lr=args.lr)\n","    elif args.optimizer == \"sgd\":\n","        optimizer = optim.SGD(\n","            model.parameters(), lr=args.lr, weight_decay=args.weight_decay\n","        )\n","    elif args.optimizer == \"momentum\":\n","        optimizer = optim.SGD(\n","            model.parameters(),\n","            lr=args.lr,\n","            momentum=args.momentum,\n","            weight_decay=args.weight_decay,\n","        )\n","    \n","    print(\n","        f\"Initialized {args.model.upper()} model with {sum(p.numel() for p in model.parameters())} \"\n","        f\"total parameters, of which {sum(p.numel() for p in model.parameters() if p.requires_grad)} are learnable.\"\n","    )\n","\n","    train_losses, valid_losses = [], []\n","    train_accs, valid_accs = [], []\n","    train_times, valid_times = [], []\n","    \n","    # We define a set of data loaders that we can use for various purposes later.\n","    train_dataloader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True, drop_last=True, pin_memory=True, num_workers=4)\n","    valid_dataloader = DataLoader(val_set, batch_size=args.batch_size, shuffle=False, drop_last=False, num_workers=4)\n","    test_dataloader = DataLoader(test_set, batch_size=args.batch_size, shuffle=False, drop_last=False, num_workers=4)\n","    for epoch in range(args.epochs):\n","        tqdm.write(f\"====== Epoch {epoch} ======>\")\n","        loss, acc, wall_time = train(epoch, model, train_dataloader, optimizer,args)\n","        train_losses.append(loss)\n","        train_accs.append(acc)\n","        train_times.append(wall_time)\n","\n","        loss, acc, wall_time = evaluate(epoch, model, valid_dataloader,args)\n","        valid_losses.append(loss)\n","        valid_accs.append(acc)\n","        valid_times.append(wall_time)\n","\n","    test_loss, test_acc, test_time = evaluate(\n","        epoch, model, test_dataloader, args, mode=\"test\"\n","    )\n","    print(f\"===== Best validation Accuracy: {max(valid_accs):.3f} =====>\")\n","\n","    # Save log if logdir provided\n","    if args.logdir is not None:\n","        print(f'Writing training logs to {args.logdir}...')\n","        os.makedirs(args.logdir, exist_ok=True)\n","        with open(os.path.join(args.logdir, 'results.json'), 'w') as f:\n","            f.write(json.dumps(\n","                {\n","                    \"train_losses\": train_losses,\n","                    \"valid_losses\": valid_losses,\n","                    \"train_accs\": train_accs,\n","                    \"valid_accs\": valid_accs,\n","                    \"test_loss\": test_loss,\n","                    \"test_acc\": test_acc\n","                },\n","                indent=4,\n","            ))\n","    \n","        # Visualize\n","        if args.visualize and args.model in ['resnet18', 'mlpmixer']:\n","            model.visualize(args.logdir)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZyJPWO1ppcTx"},"outputs":[],"source":["# Example to run MLP with 15 epochs\n","config = Arguments(model='mlp', \n","                   model_config='assignment/model_configs/mlp.json', \n","                   epochs=10, logdir=\"exps/mlp_default\")\n","main_entry(config)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"py38","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.2 (default, Feb 26 2020, 14:31:49) \n[GCC 6.3.0 20170516]"},"pycharm":{"stem_cell":{"cell_type":"raw","metadata":{"collapsed":false},"source":[]}},"vscode":{"interpreter":{"hash":"81c564fb939afe7b3f114d194e01dc23538f9aaa81b9a9b61cd5d8751a87bdce"}},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}